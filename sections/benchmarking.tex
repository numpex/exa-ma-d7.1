%!TEX root = ../exa-ma-d7.1.tex

\chapter{Benchmarking Methodology}
\label{chap:comprehensive-methodology}

\section{Introduction}
\label{sec:comprehensive-methodology-intro}

This chapter outlines a comprehensive methodology for benchmarking, testing, and validating the software developed within the \exama project. It covers the essential processes involved in ensuring that the software is optimized for exascale performance while maintaining accuracy, reliability, and efficiency across diverse hardware architectures, including CPUs, GPUs, and hybrid systems.

The methodology integrates several core components:
\begin{itemize}
    \item \textbf{Testing and Validation:} A structured process for non-regression testing, verification, and validation, ensuring that new software iterations do not degrade performance or introduce errors, while validating enhancements in real-world scenarios.
    \item \textbf{Benchmarking Strategy:} A phased benchmarking approach designed to measure the performance, scalability, and energy efficiency of the software, from baseline performance to optimized scalability testing.
    \item \textbf{Profiling and Measurement:} The use of advanced profiling and performance measurement tools like Extrae, Score-P, TAU, Vampir, and Nsight to gather detailed insights into software performance across different architectures.
    \item \textbf{Continuous Integration and Deployment (CI/CD):} The integration of continuous benchmarking and regression testing into a CI/CD pipeline ensures reproducibility, portability, and sustained high-performance across different HPC systems.
    \item \textbf{Containerization and Packaging:} The use of packaging and containerization technologies such as Spack, Guix-HPC, Docker, and Apptainer/Singularity enables reproducible and portable execution environments across various hardware platforms, simplifying deployment and reproducibility of the benchmarks.
\end{itemize}

Through this methodology, the \exama project aims to ensure that its software is exascale-ready, meeting performance and scalability expectations while maintaining scientific rigor. The following sections will delve into the key objectives, types of benchmarking, tools, and processes of this benchmarking framework.

\section{Methodology}
\label{sec:bench:methodology}


\subsection{Key Objectives}
\label{sec:key-objectives}

The objectives of the Work Package 7 are designed to 
\begin{itemize}
    \item \textbf{Developing Software:} Ensure that the software developed in the \exama project is optimized for exascale environments, from simple to advanced cases.
    \item \textbf{Testing and Benchmarking:} Implement comprehensive benchmarking processes to verify that the software meets exascale capabilities and tackles identified computational challenges.
    \item \textbf{Delivering Software Packages:} Deliver software in a standardized and reproducible form, integrating CI/CD/CB practices as proposed by \exadi.
    \item \textbf{Coordinating Co-Design:} Collaborate with \exadi and across \exama work packages to integrate co-design efforts for efficient development.
    \item \textbf{Creating a Showroom:} Establish a showroom of \exama results, presenting real-world achievements and use cases.
    \item \textbf{Building Training Material:} Develop educational resources based on the results and methodologies adopted in the \exama project, promoting knowledge transfer.
\end{itemize}
Setting up and executing an efficient benchmarking and testing methdology is an important part of the activity. 
The key objectives are:
\begin{itemize}
    \item \textbf{Performance and Scalability:} Evaluate how well software scales and performs across different architectures, from CPUs to GPUs and hybrid environments.
    \item \textbf{Energy Efficiency:} Measure energy consumption and optimize for energy-efficient performance, which is critical in exascale computing.
    \item \textbf{Reproducibility and Portability:} Ensure that benchmarks and software executions can be reproduced and run across different HPC systems.
    \item \textbf{Non-Regression:} Guarantee that future software iterations do not regress in performance or accuracy.
    \item \textbf{Validation and Verification:} Verify that software behaves correctly and validate its performance in real-world use cases.
\end{itemize}

\subsection{Testing Processes and Non-Regression Strategies}
\label{sec:testing-processes}

In the context of exascale computing, it is critical to ensure that new developments do not degrade the performance or functionality of existing software. To this end, the \exama project follows three main processes:

\subsubsection{Non-Regression Testing}
\label{sec:non-regression}

The goal of non-regression testing is to guarantee that the results produced by a specific version of the software are identical to those of previous versions, at least for functionalities that should remain unaffected by new developments.

\paragraph{Process:}
\begin{itemize}
    \item Ensure identical results on unchanged code paths or configurations.
    \item Perform these tests on simple cases that involve a limited subset of the software sources, referred to as elementary tests.
    \item Maintain consistency in the outcomes, ensuring that simple functionalities behave as expected across updates unless justified otherwise.
\end{itemize}

\subsubsection{Verification}
\label{sec:verification}

Verification tests aim to confirm that optimizations or algorithmic improvements do not lead to unacceptable deviations from known solutions.

\paragraph{Process:}
\begin{itemize}
    \item Measure the absence of drift in consolidated solutions, especially after performance optimizations.
    \item Apply elementary or level 1 integrated tests, involving few additional software sources.
    \item Introduce an acceptance criterion based on relative deviations in results.
\end{itemize}

\subsubsection{Validation}
\label{sec:validation}

Validation tests ensure that new methods or algorithms proposed within \exama deliver meaningful improvements in real-world, production-level scenarios.

\paragraph{Process:}
\begin{itemize}
    \item Evaluate the proposed algorithm in a realistic or representative configuration of the target scientific problem.
    \item Integrate validation cases with well-defined criteria to measure the contribution of the new method, whether in terms of accuracy, performance, or scalability.
\end{itemize}

\subsection{Demonstrator Framework}
\label{sec:demonstrators}

Each demonstrator in the \exama project must be comprehensively documented and subjected to the following criteria:

\subsubsection{Demonstrator Details}
\begin{itemize}
    \item \textbf{Name and Description:} Briefly describe the demonstrator, including its scientific significance and the computational barriers it seeks to overcome.
    \item \textbf{Software Components:} List the software tools involved and the challenges faced, including bottlenecks (memory, algorithms, data management).
    \item \textbf{Associated Work Packages:} Identify the work packages relevant to the demonstrator.
\end{itemize}

\subsubsection{Test Classification}
Each demonstrator must define a set of tests:
\begin{itemize}
    \item \textbf{Test Name:} Specify the name and type of the test (non-regression, verification, or validation).
    \item \textbf{Hardware Platform:} CPU, GPU, or hybrid system.
    \item \textbf{Outputs and Tolerances:} Define the expected outputs and the acceptable deviations from the reference solution.
\end{itemize}

\paragraph{Demonstrator Levels:}
\begin{itemize}
    \item \textbf{Level 1:} Involves one or two work packages (WP1, WP2, etc.).
    \item \textbf{Level 2:} Spans three or four work packages.
    \item \textbf{Level 3:} Covers the entire scope of \exama, potentially involving all work packages.
\end{itemize}

\subsection{Containerization and Software Packaging}
\label{sec:containerization}

To ensure reproducibility and simplify the deployment process, each demonstrator will be packaged using container technologies like Docker, Apptainer, Spack, or Guix-HPC. The process is as follows:

\paragraph{Container Image:}
Each demonstrator will have its own container image containing:
\begin{itemize}
    \item The operating system and necessary dependencies.
    \item Instructions for retrieving the demonstrator source code.
    \item Compilation and installation commands for the demonstrator.
    \item Execution of tests to verify correct behavior.
\end{itemize}

This approach ensures that software can be deployed in a variety of environments without worrying about system-specific dependencies, simplifying both testing and scaling efforts.

\subsection{Reproducibility and Sustainability}
\label{sec:reproducibility}

One of the core principles of the \exama project is ensuring long-term reproducibility and sustainability of the software solutions produced. This is achieved through the following processes:
\begin{itemize}
    \item CI/CD pipelines.
    \item Continuous benchmarking and regression testing on new hardware platforms.
    \item Ensuring that the new methods provide measurable improvements over time.
    \item Cross-verification of software across multiple \exama and \numpex work packages.
\end{itemize}

\subsection{Conclusion}
\label{sec:comprehensive-methodology-conclusion}

This chapter has presented a detailed methodology for benchmarking, testing, and validating software developed within \exama. It builds on core principles such as non-regression, verification, and validation, while incorporating modern CI/CD, packaging and containerization practices and continuous benchmarking. 
This is an initial version that will certainly evolve. 
\exama will implement it during the coming month and report its success in the next version of the deliverable.
Future versions of this methodology will evolve based on the experiences gained through ongoing development and real-world application of the \exama software.

\section{Concretization and Technologies}
\label{sec:benchmark:technologies}

We now turn to some concrete technologies that will help executing the Methodology.

\subsection{Types of Benchmarking}
\label{sec:methodology-types}

\paragraph{CPU or GPU}
Software that can run on either CPU or GPU but not both simultaneously (e.g., PyTorch, SciMba). In this case, benchmarks will be performed on both CPU and GPU, but no single run will execute on both architectures simultaneously. Some benchmarks may be CPU-only, others GPU-only, depending on the software's capabilities.

\paragraph{CPU and GPU}
Software that supports simultaneous execution on both CPU and GPU during a single simulation run (e.g., TRUST). Benchmarks in this category will involve executing components on both CPU and GPU, ensuring that at least one computational component runs on each architecture.

\paragraph{Explanation of Benchmarking Criteria}
\begin{description}
    \item[Pure Performance Benchmarks] Description of benchmarks based on computational performance, including metrics such as execution time and FLOPS.

    \item[Scalability Benchmarks] Discussion of scalability tests, including weak and strong scaling tests, and their relevance for exascale applications.
    \item[Energy Efficiency Benchmarks]Methodologies for measuring energy efficiency, energy profiling tools, and associated metrics (e.g., energy consumption per operation).
    \item[Hybrid Benchmarks] Strategies for benchmarking hybrid CPU-GPU codes, including tools for profiling and performance measurement.
    
\end{description}

\subsection{Profiling and Performance Measurement Tools}
\label{sec:methodology-tools}
This section presents the tools used to collect profiling data and analyze the performance of codes. 

A comparison of the advantages of each tool for different types of benchmarks is also provided.



\subsubsection{Extrae for CPU Architectures}
\label{sec:methodology-tools-extrae}

\textbf{Extrae} is a performance analysis tool developed by the Barcelona Supercomputing Center (BSC) designed to instrument and profile parallel applications. 
It is particularly useful for applications using MPI, OpenMP, CUDA, OpenCL, or hybrid programming models. 
Extrae collects detailed execution traces of an application, which can then be visualized and analyzed using tools like Paraver or Vampir~\ref{sec:methodology-tools-vampir}.
 This helps developers identify performance bottlenecks, communication inefficiencies, and parallelization issues.

The key features of Extrae include:

\begin{itemize}
  \item \textbf{Support for Multiple Programming Models:} Extrae can instrument applications using MPI, OpenMP, CUDA, OpenCL, pthreads, and combinations thereof.
  \item \textbf{Detailed Tracing:} Captures function calls, communication events, synchronization points, and hardware counters.
  \item \textbf{Customization:} Users can specify what events to trace and which metrics to collect through configuration files.
  \item \textbf{Integration with Visualization Tools:} Generates trace files compatible with Paraver and Vampir for in-depth analysis.
\end{itemize}


\subsubsection{Score-P}
\label{sec:methodology-tools-scorep}

\begin{itemize}
    \item CPU: Score-P is extensively used for CPU performance profiling, particularly in distributed memory systems using MPI, OpenMP, or hybrid programming models.
    \item GPU: Score-P also supports GPU profiling, including CUDA and OpenCL applications. It can capture the performance of both the host (CPU) and the device (GPU), including offloading, kernel execution times, and memory transfers.
\end{itemize}


\subsubsection{TAU}
\label{sec:methodology-tools-tau}

\begin{itemize}
    \item CPU: TAU is widely used for profiling and tracing CPU-bound applications, with support for both shared and distributed memory parallelism (e.g., OpenMP, MPI).
    \item GPU: TAU supports GPU profiling, particularly for CUDA-based applications. It collects performance metrics from both the host (CPU) and the device (GPU), allowing for a comprehensive analysis of hybrid codes. TAU supports CUDA, OpenCL, and OpenACC, making it suitable for GPU-accelerated applications.
\end{itemize}


\subsubsection{Vampir}
\label{sec:methodology-tools-vampir}

\begin{itemize}
    \item CPU: Vampir is a popular tool for visualizing performance data collected from CPU-based applications, particularly those running in parallel using MPI and OpenMP.
    \item GPU: Vampir can also visualize GPU-related performance data when combined with Score-P, as Score-P collects traces from both the CPU and the GPU. It can visualize events such as kernel execution, memory transfers, and CUDA API calls.
\end{itemize}


\subsubsection{Nsight for GPU Architectures}
\label{sec:methodology-tools-nsight}

Nsight\footnote{\url{https://developer.nvidia.com/nsight-systems}} is a set of tools developed by NVIDIA for profiling and debugging on GPU architectures. 
It allows for a detailed performance analysis of CUDA-based codes, including metrics such as occupancy, execution time, and memory throughput.
Nsight also provides visualizations that help pinpoint bottlenecks in GPU applications.

\subsubsection{Arm Map for CPU Architectures}
\label{sec:methodology-tools-armmap}

Arm Map\footnote{\url{https://developer.arm.com/documentation/102732/latest/}} is a lightweight and highly scalable profiler designed specifically for CPU architectures. 
It provides insights into time spent in computation, communication, and memory access. 
Arm Map can visualize CPU-bound performance bottlenecks and assist in optimizing codes for multi-core CPU systems.

\subsubsection{PETSc \texttt{-log\_view} for PETSc-based Codes}
\label{sec:methodology-tools-petsc}

PETSc~\cite{balay_petsc_2024} provides built-in profiling options via the \texttt{-log\_view} flag~\cite{balay_petsctao_2024}. 
This option enables users to gather detailed performance metrics such as function timings, memory usage, and communication patterns for codes based on the PETSc library. 

For CPU-based codes, \texttt{-log\_view} captures the performance data related to CPU function calls and overall computation, including the performance of solvers, preconditioners, and communication overhead in parallel systems.

For GPU-based codes, the \texttt{-log\_view\_gpu\_time} option is used to gather profiling information specifically for GPU activities. 
This flag tracks kernel execution times, memory transfers between host and device, and other GPU-related performance metrics, allowing for in-depth analysis of GPU-accelerated PETSc applications.




\subsection{Regression Testing and Verification}
\label{sec:methodology-regression}

\subsubsection{ReFrame}
\label{sec:methodology-regression-reframe}


ReFrame~\cite{karakasis_reframe-hpcreframe_2024} is a regression testing and benchmarking framework specifically designed for high-performance computing (HPC) systems. 
Developed and maintained by the Swiss National Supercomputing Centre (CSCS), it is optimized for managing system tests and performance benchmarks. 
ReFrame provides an abstraction layer that decouples the test logic from the specifics of the system environment, allowing seamless execution of benchmarks across multiple HPC platforms.

\subsubsection{Key Features and Design}

ReFrame's core functionality is centered around its modular, Python-based architecture, which provides extensive flexibility and control over test configurations and execution pipelines. The main features of ReFrame include:

\begin{itemize}
    \item \textbf{System Abstraction:} ReFrame abstracts system-specific details such as compilers, libraries, and modules, enabling users to define system-independent test logic. This abstraction simplifies portability across various HPC environments.
    \item \textbf{Python-Based Test Definitions:} All test logic in ReFrame is defined in Python, allowing for high customizability and the use of standard Python libraries. Tests are implemented as Python functions, which manage the execution of benchmarks, result validation, and post-processing.
    \item \textbf{Configuration and Execution Separation:} The framework separates the system configuration from the test logic. The configuration file defines system properties, including partitions, compilers, and runtime environments. The test file, on the other hand, defines the logic of the tests, including the execution stages and result handling.
    \item \textbf{Pipeline Flexibility:} ReFrame supports the creation of complex test pipelines through its modular structure. It allows defining pre- and post-processing stages, intra-test dependencies, and validation steps, making it suitable for a wide range of benchmarking scenarios.
\end{itemize}


The proposal is to use ReFrame  to benchmark the project's software and libraries on a variety of HPC systems. 
The framework’s ability to provide system-independent test logic is crucial in this respect, as the project aims to deploy and benchmark  on multiple heterogeneous HPC systems. 
This ensures consistent benchmarking across platforms with varying architectures, compilers, and parallel frameworks.

The test structure in ReFrame consists of the following key components:

\begin{itemize}
    \item \textbf{Configuration File:} Defines the HPC systems and their environments, including compilers, libraries, and runtime modules. A new configuration file or an extension to an existing one is required when deploying tests on a different system.
    \item \textbf{Test File:} Contains the core test logic, including benchmark execution and validation stages. The test file remains system-independent, allowing the same test to be deployed on multiple platforms.
    \item \textbf{Post-Processing:} ReFrame provides built-in support for result collection and validation. Benchmark results are parsed and stored in a standardized format, facilitating consistent performance reporting across different HPC environments.
\end{itemize}

ReFrame ensures reproducibility, scalability, and portability in Exa-MA benchmarking efforts, making it thus a central tool in managing the project’s performance evaluation across various supercomputing resources.


\subsection{Packaging and Containerization}
\label{sec:methodology-packaging}

\subsubsection{Spack}
\label{sec:methodology-packaging-spack}

Spack~\cite{gamblin_spack_2015} is a flexible package management tool used to simplify the installation of complex HPC software environments. It allows users to create and manage multiple versions of software libraries and applications, ensuring compatibility across different systems.

\subsubsection{Guix-HPC}
\label{sec:methodology-packaging-guix-hpc}

Guix-HPC~\cite{vallet_toward_2022}, an advanced package management system tailored for HPC environments. Guix-HPC ensures reproducibility and scalability by providing a declarative environment specification. By using Guix, we can control software dependencies and build environments across different HPC systems, ensuring that benchmarks and experiments can be replicated with exact configurations.


\subsubsection{Containers}
\label{sec:methodology-packaging-container}

Methods of containerization to ensure the portability and reproducibility of benchmarking environments.
The use of Docker and Singularity to encapsulate dependencies and simplify deployment on different infrastructures is discussed.

\subsection{Presentation of Results}
\label{sec:methodology-presentation}

The presentation of benchmarking results is critical for clearly communicating performance metrics and insights derived from the analysis. Standard formats for presenting these results include the following:

\paragraph{Performance Graphs:} Performance graphs such as line plots, bar charts, and scatter plots are used to visualize key metrics like execution time, floating-point operations per second (FLOPS), memory usage, and scalability (e.g., strong and weak scaling). These graphs provide a visual comparison of the software's performance across different architectures (CPU, GPU, or hybrid setups), problem sizes, and parallel configurations. Graphs may also be used to display energy efficiency metrics, such as power consumption over time or energy-to-solution.

\paragraph{Comparative Tables:} Comparative tables are employed to present side-by-side comparisons of different software versions or configurations. These tables can include metrics such as execution time, memory usage, cache utilization, FLOPS, and energy consumption. Comparative tables offer a quick overview of how different systems, algorithms, or configurations perform under the same conditions.

\paragraph{Summary Reports:} Summary reports encapsulate the key findings of the benchmarking activities. These reports include both qualitative and quantitative evaluations of performance, scalability, and energy efficiency. A summary report typically includes a discussion of bottlenecks, identified challenges, and opportunities for optimization. Additionally, tables and graphs from the benchmarking process are integrated into the report for the analysis of the results.


\subsection{Scalability and Hardware Environments}
\label{sec:methodology-environments}

This section describes the hardware architectures used in the benchmarks:
\begin{itemize}
    \item \textbf{CPU Architectures:} Multi-core CPU systems (Intel, AMD, ARM) with associated performance metrics such as scalability, memory bandwidth, and computational throughput.
    \item \textbf{GPU Architectures:} NVIDIA and AMD GPUs, focusing on metrics such as memory latency, occupancy, and computational throughput using CUDA or HIP.
    \item \textbf{Hybrid Systems:} Systems combining both CPU and GPU architectures, analyzing the balance of workloads between the two and exploring tools that optimize for such hybrid environments.
\end{itemize}
Tools for resource management, such as job scheduling systems (e.g., SLURM), are also discussed.

\section{Conclusion}
\label{sec:Technology-conclusion}

In this chapter, we have introduced the practical tools, methodologies, and technologies essential for executing the benchmarking activities within the \exama project. 
The systematic categorization of software into CPU-only, GPU-only, or hybrid CPU-GPU execution modes allows for targeted benchmarking strategies that reflect the diverse architectures available in modern high-performance computing (HPC) environments and the software identified in \exama.

We further detailed the phased benchmarking approach, which moves from baseline performance evaluation to optimization and scalability testing. The use of specialized tools such as Extrae, Score-P, TAU, Vampir, and Nsight ensures comprehensive performance profiling across different architectures. Additionally, regression testing frameworks like ReFrame provide a consistent and scalable solution for ensuring reproducibility across a variety of systems.

Lastly, the packaging and containerization strategies, supported by tools like Spack, Guix-HPC, Docker, and Apptainer/Singularity, will play a key role in maintaining portability, reproducibility, and long-term sustainability of the software environments and enable continuous benchmarking on HPC Systems.

Together, these technologies and methodologies create a robust framework for evaluating and optimizing the exascale readiness of software developed in the \exama project. This approach not only ensures high performance but also prepares the project for future advancements in HPC infrastructures.