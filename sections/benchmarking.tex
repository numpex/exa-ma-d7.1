%!TEX root = ../exa-ma-d7.1.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Guidelines
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Benchmarking Methodology}
\label{chap:methodology}

\section{Introduction}
\label{sec:methodology-intro}

This chapter describes the methodological aspects related to the benchmarking activities in the Exa-MA project. 
This methodology covers the types of benchmarks performed, the presentation of results, the tools used for profiling and performance measurement, as well as the regression testing and packaging strategies to ensure reproducibility and portability of the code.

\section{Types of Benchmarking}
\label{sec:methodology-types}

\subsection{Architectures}

In the context of the D7.1 deliverable, it is essential to clarify the supported features across different hardware configurations, specifically CPUs, GPUs, and hybrid setups. 
The following classification describes the architectural capabilities that will be benchmarked:

\paragraph{CPU Only}
Software that will exclusively run on CPU architectures (e.g., CGAL, Freefem++, Feel++, Manta). Benchmarks for these codes will only target CPU architectures.

\paragraph{GPU Only}
Software that will exclusively run on GPU architectures (e.g., Zellij). Benchmarks for these codes will focus solely on GPU architectures.

\paragraph{CPU or GPU}
Software that can run on either CPU or GPU but not both simultaneously (e.g., PyTorch, SciMba). In this case, benchmarks will be performed on both CPU and GPU, but no single run will execute on both architectures simultaneously. Some benchmarks may be CPU-only, others GPU-only, depending on the software's capabilities.

\paragraph{CPU and GPU}
Software that supports simultaneous execution on both CPU and GPU during a single simulation run (e.g., TRUST). Benchmarks in this category will involve executing components on both CPU and GPU, ensuring that at least one computational component runs on each architecture.

\paragraph{Explanation of Benchmarking Criteria}
\begin{itemize}
    \item \textbf{CPU Only:} If selected, benchmarks will be performed exclusively on CPU architectures.
    \item \textbf{GPU Only:} If selected, benchmarks will focus exclusively on GPU architectures.
    \item \textbf{CPU and GPU:} If selected, benchmarks will involve simultaneous execution on CPU and GPU within the same run.
    \item \textbf{CPU or GPU:} If selected, benchmarks will involve execution on either CPU or GPU, but not simultaneously. Some benchmarks will target CPU, others GPU, depending on the computational components.
\end{itemize}


\subsection{Pure Performance Benchmarks}
\label{sec:methodology-types-performance}
Description of benchmarks based on computational performance, including metrics such as execution time and FLOPS.

\subsection{Scalability Benchmarks}
\label{sec:methodology-types-scalability}

Discussion of scalability tests, including weak and strong scaling tests, and their relevance for exascale applications.

\subsection{Energy Efficiency Benchmarks}
\label{sec:methodology-types-energy}

Methodologies for measuring energy efficiency, energy profiling tools, and associated metrics (e.g., energy consumption per operation).

\section{Profiling and Performance Measurement Tools}
\label{sec:methodology-tools}

This section presents the tools used to collect profiling data and analyze the performance of codes. 
A comparison of the advantages of each tool for different types of benchmarks is also provided.

\subsection{Extrae for CPU Architectures}
\label{sec:methodology-tools-extrae}

\subsubsection{Score-P}
\label{sec:methodology-tools-scorep}

\begin{itemize}
    \item CPU: Score-P is extensively used for CPU performance profiling, particularly in distributed memory systems using MPI, OpenMP, or hybrid programming models.
    \item GPU: Score-P also supports GPU profiling, including CUDA and OpenCL applications. It can capture the performance of both the host (CPU) and the device (GPU), including offloading, kernel execution times, and memory transfers.
\end{itemize}


\subsection{TAU}
\label{sec:methodology-tools-tau}

\begin{itemize}
    \item CPU: TAU is widely used for profiling and tracing CPU-bound applications, with support for both shared and distributed memory parallelism (e.g., OpenMP, MPI).
    \item GPU: TAU supports GPU profiling, particularly for CUDA-based applications. It collects performance metrics from both the host (CPU) and the device (GPU), allowing for a comprehensive analysis of hybrid codes. TAU supports CUDA, OpenCL, and OpenACC, making it suitable for GPU-accelerated applications.
\end{itemize}


\subsection{Vampir}
\label{sec:methodology-tools-vampir}

\begin{itemize}
    \item CPU: Vampir is a popular tool for visualizing performance data collected from CPU-based applications, particularly those running in parallel using MPI and OpenMP.
    \item GPU: Vampir can also visualize GPU-related performance data when combined with Score-P, as Score-P collects traces from both the CPU and the GPU. It can visualize events such as kernel execution, memory transfers, and CUDA API calls.
\end{itemize}


\subsection{Nsight for GPU Architectures}
\label{sec:methodology-tools-nsight}

Nsight\footnote{\url{https://developer.nvidia.com/nsight-systems}} is a set of tools developed by NVIDIA for profiling and debugging on GPU architectures. 
It allows for a detailed performance analysis of CUDA-based codes, including metrics such as occupancy, execution time, and memory throughput.
Nsight also provides visualizations that help pinpoint bottlenecks in GPU applications.

\subsection{Arm Map for CPU Architectures}
\label{sec:methodology-tools-armmap}

Arm Map\footnote{\url{https://developer.arm.com/documentation/102732/latest/}} is a lightweight and highly scalable profiler designed specifically for CPU architectures. 
It provides insights into time spent in computation, communication, and memory access. 
Arm Map can visualize CPU-bound performance bottlenecks and assist in optimizing codes for multi-core CPU systems.

\subsection{PETSc \texttt{-log\_view} for PETSc-based Codes}
\label{sec:methodology-tools-petsc}

PETSc~\cite{balay_petsc_2024} provides built-in profiling options via the \texttt{-log\_view} flag~\cite{balay_petsctao_2024}. 
This option enables users to gather detailed performance metrics such as function timings, memory usage, and communication patterns for codes based on the PETSc library. 

For CPU-based codes, \texttt{-log\_view} captures the performance data related to CPU function calls and overall computation, including the performance of solvers, preconditioners, and communication overhead in parallel systems.

For GPU-based codes, the \texttt{-log\_view\_gpu\_time} option is used to gather profiling information specifically for GPU activities. 
This flag tracks kernel execution times, memory transfers between host and device, and other GPU-related performance metrics, allowing for in-depth analysis of GPU-accelerated PETSc applications.




\section{Regression Testing and Verification}
\label{sec:methodology-regression}

\subsection{ReFrame}
\label{sec:methodology-regression-reframe}


ReFrame~\cite{karakasis_reframe-hpcreframe_2024} is a regression testing and benchmarking framework specifically designed for high-performance computing (HPC) systems. 
Developed and maintained by the Swiss National Supercomputing Centre (CSCS), it is optimized for managing system tests and performance benchmarks. 
ReFrame provides an abstraction layer that decouples the test logic from the specifics of the system environment, allowing seamless execution of benchmarks across multiple HPC platforms.

\subsubsection{Key Features and Design}

ReFrame's core functionality is centered around its modular, Python-based architecture, which provides extensive flexibility and control over test configurations and execution pipelines. The main features of ReFrame include:

\begin{itemize}
    \item \textbf{System Abstraction:} ReFrame abstracts system-specific details such as compilers, libraries, and modules, enabling users to define system-independent test logic. This abstraction simplifies portability across various HPC environments.
    \item \textbf{Python-Based Test Definitions:} All test logic in ReFrame is defined in Python, allowing for high customizability and the use of standard Python libraries. Tests are implemented as Python functions, which manage the execution of benchmarks, result validation, and post-processing.
    \item \textbf{Configuration and Execution Separation:} The framework separates the system configuration from the test logic. The configuration file defines system properties, including partitions, compilers, and runtime environments. The test file, on the other hand, defines the logic of the tests, including the execution stages and result handling.
    \item \textbf{Pipeline Flexibility:} ReFrame supports the creation of complex test pipelines through its modular structure. It allows defining pre- and post-processing stages, intra-test dependencies, and validation steps, making it suitable for a wide range of benchmarking scenarios.
\end{itemize}


The proposal is to use ReFrame  to benchmark the project's software and libraries on a variety of HPC systems. 
The framework’s ability to provide system-independent test logic is crucial in this respect, as the project aims to deploy and benchmark  on multiple heterogeneous HPC systems. 
This ensures consistent benchmarking across platforms with varying architectures, compilers, and parallel frameworks.

The test structure in ReFrame consists of the following key components:

\begin{itemize}
    \item \textbf{Configuration File:} Defines the HPC systems and their environments, including compilers, libraries, and runtime modules. A new configuration file or an extension to an existing one is required when deploying tests on a different system.
    \item \textbf{Test File:} Contains the core test logic, including benchmark execution and validation stages. The test file remains system-independent, allowing the same test to be deployed on multiple platforms.
    \item \textbf{Post-Processing:} ReFrame provides built-in support for result collection and validation. Benchmark results are parsed and stored in a standardized format, facilitating consistent performance reporting across different HPC environments.
\end{itemize}

ReFrame ensures reproducibility, scalability, and portability in Exa-MA benchmarking efforts, making it thus a central tool in managing the project’s performance evaluation across various supercomputing resources.


\section{Packaging and Containerization}
\label{sec:methodology-packaging}

\subsection{Spack}
\label{sec:methodology-packaging-spack}

Spack~\cite{gamblin_spack_2015} is a flexible package management tool used to simplify the installation of complex HPC software environments. It allows users to create and manage multiple versions of software libraries and applications, ensuring compatibility across different systems.

\subsection{Guix-HPC}
\label{sec:methodology-packaging-guix-hpc}

Guix-HPC~\cite{vallet_toward_2022}, an advanced package management system tailored for HPC environments. Guix-HPC ensures reproducibility and scalability by providing a declarative environment specification. By using Guix, we can control software dependencies and build environments across different HPC systems, ensuring that benchmarks and experiments can be replicated with exact configurations.


\subsection{Containers}
\label{sec:methodology-packaging-container}

Methods of containerization to ensure the portability and reproducibility of benchmarking environments.
The use of Docker and Singularity to encapsulate dependencies and simplify deployment on different infrastructures is discussed.

\section{Presentation of Results}
\label{sec:methodology-presentation}
Standard formats for presenting benchmarking results, including performance graphs, comparative tables, and summary reports of performance tests.

\section{Scalability and Hardware Environments}
\label{sec:methodology-environments}

This section describes the hardware architectures used in the benchmarks:
\begin{itemize}
    \item \textbf{CPU Architectures:} Multi-core CPU systems (Intel, AMD, ARM) with associated performance metrics such as scalability, memory bandwidth, and computational throughput.
    \item \textbf{GPU Architectures:} NVIDIA and AMD GPUs, focusing on metrics such as memory latency, occupancy, and computational throughput using CUDA or HIP.
    \item \textbf{Hybrid Systems:} Systems combining both CPU and GPU architectures, analyzing the balance of workloads between the two and exploring tools that optimize for such hybrid environments.
\end{itemize}
Tools for resource management, such as job scheduling systems (e.g., SLURM), are also discussed in this section.

\section{Conclusion}
\label{sec:methodology-conclusion}

A summary of the key methodological points and recommendations for future iterations of benchmarking in the context of the Exa-MA project.