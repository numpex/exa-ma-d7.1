\section{Software: Freefem++}
\label{sec:WP3:Freefem++:software}

\begin{table}[h!]
    \centering
    { \setlength{\parindent}{0pt}
    \def\arraystretch{1.25}
    \arrayrulecolor{numpexgray}
    {\fontsize{9}{11}\selectfont
    \begin{tabular}{!{\color{numpexgray}\vrule}p{.4\textwidth}!{\color{numpexgray}\vrule}p{.6\textwidth}!{\color{numpexgray}\vrule}}
        \rowcolor{numpexgray}{\rule{0pt}{2.5ex}\color{white}\bf Field} & {\rule{0pt}{2.5ex}\color{white}\bf Details} \\
        \rowcolor{white}\textbf{Consortium} & \begin{tabular}{l}
Sorbonne U\\
\end{tabular} \\
        \rowcolor{numpexlightergray}\textbf{Exa-MA Partners} & \begin{tabular}{l}
Inria PARIS\\
Sorbonne U\\
\end{tabular} \\
        \rowcolor{white}\textbf{Contact Emails} & \begin{tabular}{l}
frederic.hecht@sorbonne-universite.fr\\
pierre-henri.tournier@sorbonne-universite.fr\\
pierre.jolivet@sorbonne-universite.fr\\
\end{tabular} \\
        \rowcolor{numpexlightergray}\textbf{Supported Architectures} & \begin{tabular}{l}
CPU Only\\
\end{tabular} \\
        \rowcolor{white}\textbf{Repository} & \href{https://github.com/FreeFem/FreeFem-sources}{https://github.com/FreeFem/FreeFem-sources} \\
        \rowcolor{numpexlightergray}\textbf{License} & \begin{tabular}{l}
OSS:: LGPL v*\\
\end{tabular} \\
        \rowcolor{white}\textbf{Bottlenecks roadmap} & \begin{tabular}{l}
B10 - Scientific Productivity\\
B11 - Reproducibility and Replicability of Computation\\
B6 - Data Management\\
B7 - Exascale Algorithms\\
\end{tabular} \\
        \bottomrule
    \end{tabular}
    }}
    \caption{WP3: Freefem++ Information}
\end{table}

\subsection{Software Overview}
\label{sec:WP3:Freefem++:summary}

In~\cref{tab:WP3:Freefem++:features} we provide a summary of the FreeFEM features relevant to the work package which are briefly discussed.

\begin{table}[h!]
    \centering
    { 
        \setlength{\parindent}{0pt}
        \def\arraystretch{1.25}
        \arrayrulecolor{numpexgray}
        {
            \fontsize{9}{11}\selectfont
            \begin{tabular}{!{\color{numpexgray}\vrule}p{.25\linewidth}!{\color{numpexgray}\vrule}p{.6885\linewidth}!{\color{numpexgray}\vrule}}
    
    \rowcolor{numpexgray}{\rule{0pt}{2.5ex}\color{white}\bf Features} &  {\rule{0pt}{2.5ex}\color{white}\bf Short Description }\\ 
    
\rowcolor{white}    algebraic multiphysics coupling & PETSc can be used within FreeFEM for the solution of algebraic multiphysics coupling problems. Moreover, the user has easy access to additional relevant information about the problem (mesh, PDE, elementary matrices) which can allow to design and test more efficient coupling algorithms informed by the physics of the problem.\\
\rowcolor{numpexlightergray}    domain decomposition methods & extensive interface to the HPDDM library, accessible both via its own FreeFEM interface and through the PETSc interface of FreeFEM. The majority of the parallel examples of the FreeFEM distribution uses HPDDM through PETSc for the solution of the linear systems. FreeFEM also provides a set of high-level FreeFEM scripts implementing domain decomposition methods called ffddm for easy prototyping.\\
\rowcolor{white}    reuse of Krylov subspaces for multiple right-hand sides & thanks to its interface to HPDDM, FreeFEM can take advantage of advanced Krylov subspace methods implementing blocking and recycling strategies for the efficient solution of linear systems with multiple right-hand sides.\\
\end{tabular}
        }
    }
    \caption{WP3: Freefem++ Features}
    \label{tab:WP3:Freefem++:features}
\end{table}


\subsection{Parallel Capabilities}
\label{sec:WP3:Freefem++:performances}

FreeFEM uses either direct or iterative solvers for the solution of the linear systems. In both cases, it is interfaced with high-performance CPU-based linear solver libraries using MPI for parallelism. For direct methods, it is interfaced with the multifrontal solver MUMPS. For large scale computations, FreeFEM relies on its extensive interface to the PETSc library. In particular, it can use the high-performance domain decomposition library HPDDM as the linear solver backend, which is available both through PETSc and through its own FreeFEM interface. After the input simulation mesh has been partitioned and distributed among the MPI processes, the linear system is naturally assembled and passed through the solver backend in a distributed way.\\
More details about scalability are given in the next section and in Section~\cref{sec:WP3:HPDDM:software}

\begin{itemize}
    \item \textbf{Parallel Environment:} MPI

    \item \textbf{Architectures:} CPU only ; local computing clusters, french supercomputers (Irene, Jean Zay, ...)
 
    \item \textbf{Scalability:} relies on high-performance solver librairies such as HPDDM and PETSc. Scalability is measured in terms of number of iterations of the iterative solver and computing time of both setup and solution steps.
\end{itemize}

\subsection{Initial Performance Metrics}
\label{sec:WP3:Freefem++:metrics}

Here we present the current state of the software concering the performance of the solution step for problems involving various physics. For large scale problems, the main concern is the efficiency of the preconditioner, both in terms of number of iterations and parallel efficiency. For problems with provably robust preconditioners such as SPD problems preconditioned by a multi-level GenEO domain-decomposition method, the scalability is very good up to several thousands CPU cores, where the increase in coarse space size starts to hinder parallel efficiency (see Section~\cref{sec:WP3:HPDDM:software}). On the other hand, there are physics for which designing provably robust and efficient solvers is still a current open challenge.\\

The first benchmark presented in this section illustrates this difficulty in the context of high-frequency electromagnetic wave propagation problems. We consider a very simple setting of scattering of a point source in a homogeneous domain. The benchmark solves the time-harmonic second-order Maxwell's equations in a cube, where the source is a gaussian point source in the center of the cube, with lowest-order absorbing boundary conditions on all 6 faces.\\
The benchmark consists in a FreeFEM script publicly available in the distribution. The script outputs the number of iterations and computing time for a specific frequency that the user can specify as a command-line parameter. The performance can be evaluated through the PETSc output logs.\\

\begin{itemize}
    \item \textbf{Overall Performance:} FreeFEM relies on high-performance solver libraries such as HPDDM and PETSc for the efficient solution of large-scale linear systems. State-of-the-art multilevel domain-decomposition solvers have been shown to be robust and scalable up to several thousand CPU cores (see Section~\cref{sec:WP3:HPDDM:software}). However, designing robust and efficient solvers for difficult physics, such as high-frequency wave propagation problems, remains an open challenge. FreeFEM is the ideal framework to try and tackle these open problems: it allows for a quick and easy description and set up of test cases involving various physics, and it is developed hand-in-hand with our research in scientific computing and parallel solvers. The FreeFEM/ffddm/HPDDM/PETSc ecosystem greatly facilitates the development of new methods, from research and prototyping all the way to HPC implementation.
    \item \textbf{Input/Output Datasets:} Currently, the dataset for each benchmark consists in the FreeFEM script used to define and solve the corresponding problem. The scripts are open acces and available in the FreeFEM distribution.
    \item \textbf{Challenges:}
        \begin{itemize}
        \item definition of the geometry of the problem: relying on the construction of a global mesh which is then partitioned into subdomains and distributed among MPI processes is impratical for large problems. This is discussed in more details in Section~\cref{sec:WP1:Freefem++:software}.
        \item for difficult physics such as high-frequency wave propagation, designing robust and efficient preconditioners is still an open challenge. In particular, in the context of multi-level domain decomposition methods, the construction of efficient coarse spaces of reasonable size can be challenging.
        \item currently, the benchmark datasets consist only in the FreeFEM script generating and solving the problem. A discussion should be made to provide the (possibly very large) linear system to solve as a benchmark input, allowing to compare results with other solver libraries and software. The price is then the loss of information about the physics of the problem, effectively discarding non-algebraic solvers.
        \end{itemize}
    \item \textbf{Future Improvements:}
        \begin{itemize}
        \item TODO parallel mesh generation
        \item TODO design robust coarse spaces for difficult physics
        \item TODO think about input datasets for solver benchmarks 
        \end{itemize}
\end{itemize}

\subsubsection{Benchmark \#1: Time-harmonic Maxwell equations at high frequency}

This benchmark consists in solving the time-harmonic second-order Maxwell's equations in the unit cube, where the source is a gaussian point source in the center of the cube, with lowest-order absorbing boundary conditions on all 6 faces. The complex amplitude of the electric field $\mathbf{E(x)}$ is solution of

\begin{equation}
  \begin{cases}
\nabla\times(\nabla\times \mathbf{E})- k^2 \mathbf{E} = \mathbf{f}, & \text{in } \Omega,\\
\nabla\times \mathbf{E}\times \textbf{n}+ \mathrm{i} k \textbf{n} \times (\mathbf{E}\times \textbf{n}) = 0 & \text{on } \partial\Omega,\\
  \end{cases}
\end{equation}

where $k = 2\pi f$, $f \in \mathbb{R}^*_+$ is the frequency, $\Omega$ is the unit cube, $\partial\Omega$ are the boundary faces of $\Omega$, $\textbf{n(x)}$ is the outward unit normal vector to $\partial\Omega$, and the source term is $\textbf{f(x)} = [0,0,\exp^{-50 k [(x-0.5)^2 + (y-0.5)^2 + (z-0.5)^2]}]$.\\
The problem is discretized with lowest-order Nedelec edge finite elements. The unit cube is discretized with a regular tetrahedral mesh (each elementary cube is cut into 6 tetrahedra, with 1 diagonal priviledged) with a constant mesh size corresponding to 10 points per wavelength $\lambda = \frac{1}{f}$.


The benchmark consists in a FreeFEM script publicly available in the distribution and can be found at~\url{https://github.com/FreeFem/FreeFem-sources/blob/master/examples/hpddm/maxwell-mg-3d-PETSc-complex.edp}. The script outputs the number of iterations and computing time for a specific frequency $f$ that the user can specify as a command-line parameter. The performance can be evaluated through the PETSc output logs. The number of iterations of the Krylov solver, the FLOPS and the execution time are reported for increasing frequencies.\\

The preconditioner used in this benchmark is a nested two-grid optimized overlapping Schwarz domain-decomposition preconditioner. The construction of the preconditioner is done using PETSc. A thorough description of the method can be found in \textcolor{red}{XXXX}. The mesh is partitioned using the automatic graph partitioner \textit{ParMETIS}. The GMRES relative tolerance is set to $10^{-5}$. Each subdomain is assigned to one MPI process. In the preconditioner, the inner relative tolerance of the nested coarse solve is set to $10^{-1}$.

\begin{table}[h!]
    \centering
    { \setlength{\parindent}{0pt}
    \def\arraystretch{1.25}
    \arrayrulecolor{numpexgray}
    {\fontsize{9}{11}\selectfont
    \begin{tabular}{!{\color{numpexgray}\vrule}c!{\color{numpexgray}\vrule}c!{\color{numpexgray}\vrule}c!{\color{numpexgray}\vrule}c!{\color{numpexgray}\vrule}c!{\color{numpexgray}\vrule}c!{\color{numpexgray}\vrule}c!{\color{numpexgray}\vrule}c!{\color{numpexgray}\vrule}c!{\color{numpexgray}\vrule}c!{\color{numpexgray}\vrule}}
        \rowcolor{numpexgray}{\color{white}\bf f} & {\color{white}\bf \# dofs} & {\color{white}\bf \# cores} & {\color{white}\bf \# iter.}  & \multicolumn{5}{c!{\color{numpexgray}\vrule}}{\color{white}\bf Computing time (s)} \\
        \rowcolor{numpexgray} & & & & {\color{white}\bf mesh part.} & {\color{white}\bf assembly} & {\color{white}\bf prec. setup} & {\color{white}\bf solve} & {\color{white}\bf VTK export} \\
        \texttt{8} & \pgfmathprintnumber{3641840} & \pgfmathprintnumber{48} & \texttt{7} & \pgfmathprintnumber{5.974709965} & \pgfmathprintnumber{4.332314352} & \pgfmathprintnumber{1.2046e+01} & \pgfmathprintnumber{2.4177e+01} & \pgfmathprintnumber{1.945695103} \\
        \rowcolor{numpexlightergray}
        \texttt{12} & \pgfmathprintnumber{12225960} & \pgfmathprintnumber{162} & \texttt{9} & \pgfmathprintnumber{8.98338665} & \pgfmathprintnumber{4.556623172} & \pgfmathprintnumber{1.2622e+01} & \pgfmathprintnumber{3.1155e+01} & \pgfmathprintnumber{2.038234012}\\
        \texttt{16} & \pgfmathprintnumber{28902880} & \pgfmathprintnumber{384} & \texttt{12} & \pgfmathprintnumber{12.99739596} & \pgfmathprintnumber{4.857938825} & \pgfmathprintnumber{1.3344e+01} & \pgfmathprintnumber{3.9628e+01} & \pgfmathprintnumber{2.140030511} \\
        \rowcolor{numpexlightergray}
        \texttt{20} & \pgfmathprintnumber{56360600} & \pgfmathprintnumber{750} & \texttt{18} & \pgfmathprintnumber{17.22267813} & \pgfmathprintnumber{5.45332545} & \pgfmathprintnumber{1.3444e+01} & \pgfmathprintnumber{5.5973e+01} & \pgfmathprintnumber{2.164726065}\\
        \texttt{24} & \pgfmathprintnumber{97287120} & \pgfmathprintnumber{1296} & \texttt{25}& \pgfmathprintnumber{20.58727531} & \pgfmathprintnumber{5.664089589} & \pgfmathprintnumber{1.3940e+01} & \pgfmathprintnumber{7.6634e+01} & \pgfmathprintnumber{2.235102781}\\
        \rowcolor{numpexlightergray}
        \texttt{28} & \pgfmathprintnumber{154370440} & \pgfmathprintnumber{2058} & \texttt{35} & \pgfmathprintnumber{29.6948684} & \pgfmathprintnumber{5.643965845} & \pgfmathprintnumber{1.3856e+01} & \pgfmathprintnumber{1.0889e+02} & \pgfmathprintnumber{2.260791771}\\
        \texttt{32} & \pgfmathprintnumber{230298560} & \pgfmathprintnumber{3072} & \texttt{45} & \pgfmathprintnumber{42.58705387} & \pgfmathprintnumber{5.691343932} & \pgfmathprintnumber{1.4165e+01} & \pgfmathprintnumber{1.4772e+02} & \pgfmathprintnumber{2.523501058}\\
    \end{tabular}
    }}
    \caption{Weak scaling experiment increasing frequency from $f=8$ to $f=32$. For each frequency we report the number of degrees of freedom, the number of cores, the number of GMRES iterations, and the computing time (in seconds) of the different steps of the simulation: mesh partitioning, assembly of the linear system, construction of the preconditioner, solution of the linear system, VTK export of the solution.}
    \label{tab:maxwell}
\end{table}

In Table~\cref{tab:maxwell} we report a weak scaling experiment, where the number of CPU cores scales with $f^3$ so as to keep the local problem sizes constant. We report the computing time of the different steps of the simulation: mesh partitioning, assembly of the linear system, construction of the preconditioner, solution of the linear system, VTK export of the solution. The frequency varies from 8 to 32, with a corresponding increase of the number of unknowns from 3.6 million to 230 million. The number of cores increases from 48 to 3072.

\textbf{Hardware and software setting:} The experiment is done on the Skylake partition of the Irene supercomputer. FreeFEM is compiled using the Intel 20.0.4 compiler suite.

The results can be summarized as follows:

\begin{itemize}
\item The parallel assembly of the linear system, the setup of the preconditioner and the export to VTK are all local computations and operations done independently on each subdomain. The local matrices are assembled on the local meshes of the subdomains, and the construction of the preconditioner mainly consists in performing the LU factorization of the two local matrices (corresponding to the fine and coarse levels) using the direct solver \textit{MUMPS}. It is then natural that the computing time corresponding to these steps stays approximately constant throughout the weak scaling experiment.
\item As discussed in Section~\cref{{sec:WP1:Freefem++:software}}, the cost of the initial partitioning of the global mesh using \textit{ParMETIS} increases. This can become a bottleneck for large scale simulations. Moreover, without relying on an initial coarsening of the global mesh, the memory cost of holding the mesh becomes intractable. This is evidenced here, where going higher than $f=32$ produces an out-of-memory error.
\item Even though the setup cost of the preconditioner is cheap and scalable, the number of iterations increases with frequency, from 7 iterations for $f=8$ to 45 iterations for $f=32$. Correspondingly, the computing time of the solution step increases from 24 to 148 seconds. The increase is a little more than linear with respect to frequency.
\end{itemize}

\subsection{12-Month Roadmap}
\label{sec:WP3:Freefem++:roadmap}

In this section, describe the roadmap for improving benchmarks and addressing the challenges identified. This should include:
\begin{itemize}
    \item \textbf{Data Improvements:} Plans for improving input/output data management, including making datasets more accessible and ensuring reproducibility through open-data initiatives.
    \item \textbf{Methodology Application:} Implementation of the benchmarking methodology proposed in this deliverable to streamline reproducibility and dataset management.
    \item \textbf{Results Retention:} Plans to maintain benchmark results in a publicly accessible repository with appropriate metadata and documentation, ensuring long-term usability.
\end{itemize}

In~\cref{tab:WP3:Freefem++:bottlenecks}, we briefly discuss the bottleneck roadmap associated to the software and relevant to the work package.

\begin{table}[h!]
    \centering
    
    

    \centering
    { 
        \setlength{\parindent}{0pt}
        \def\arraystretch{1.25}
        \arrayrulecolor{numpexgray}
        {
            \fontsize{9}{11}\selectfont
            \begin{tabular}{!{\color{numpexgray}\vrule}p{.25\linewidth}!{\color{numpexgray}\vrule}p{.6885\linewidth}!{\color{numpexgray}\vrule}}
    
    \rowcolor{numpexgray}{\rule{0pt}{2.5ex}\color{white}\bf Bottlenecks} &  {\rule{0pt}{2.5ex}\color{white}\bf Short Description }\\ 
    
\rowcolor{white}    B10 - Scientific Productivity & provide short description here \\
\rowcolor{numpexlightergray}    B11 - Reproducibility and Replicability of Computation & provide short description here \\
\rowcolor{white}    B6 - Data Management & provide short description here \\
\rowcolor{numpexlightergray}    B7 - Exascale Algorithms & provide short description here \\
\end{tabular}
        }
    }
    \caption{WP3: Freefem++ plan with Respect to Relevant Bottlenecks}
    \label{tab:WP3:Freefem++:bottlenecks}
\end{table}