\section{Software: Zellij}
\label{sec:WP5:Zellij:software}

\begin{table}[h!]
    \centering
    { \setlength{\parindent}{0pt}
    \def\arraystretch{1.25}
    \arrayrulecolor{numpexgray}
    {\fontsize{9}{11}\selectfont
    \begin{tabular}{!{\color{numpexgray}\vrule}p{.4\textwidth}!{\color{numpexgray}\vrule}p{.6\textwidth}!{\color{numpexgray}\vrule}}
        \rowcolor{numpexgray}{\rule{0pt}{2.5ex}\color{white}\bf Field} & {\rule{0pt}{2.5ex}\color{white}\bf Details} \\
        \rowcolor{white}\textbf{Consortium} & \begin{tabular}{l}
Université de Lille\\
\end{tabular} \\
        \rowcolor{numpexlightergray}\textbf{Exa-MA Partners} & \begin{tabular}{l}
Inria Lille\\
\end{tabular} \\
        \rowcolor{white}\textbf{Contact Emails} & \begin{tabular}{l}
el-ghazali.talbi@univ-lille.fr, thomas.firmin@univ-lille.fr \\
\end{tabular} \\
        \rowcolor{numpexlightergray}\textbf{Supported Architectures} & \begin{tabular}{l}
CPU Only \\
\end{tabular} \\
        \rowcolor{white}\textbf{Repository} & \href{https://github.com/ThomasFirmin/zellij}{https://github.com/ThomasFirmin/zellij} \\
        \rowcolor{numpexlightergray}\textbf{License} & \begin{tabular}{l}
OSS: Cecill-*\\
\end{tabular} \\
        \rowcolor{white}\textbf{Bottlenecks roadmap} & \begin{tabular}{l}
B7 - Exascale Algorithms\\
B9 - Resilience, robustness and accuracy \\
B10 - Scientific Productivity\\
\end{tabular} \\
        \bottomrule
    \end{tabular}
    }}
    \caption{WP5: Zellij Information}
\end{table}

\subsection{Software Overview}
\label{sec:WP5:Zellij:summary}

Zellij is a {\it fractal-based decomposition algorithm} for solving optimization problems. Zellij is organized around five search components: fractal geometry, tree search, scoring, exploration and exploitation. It decomposes the search space via a given \textit{fractal geometrical object}. {\it Tree search} allows a dynamic and hierarchical fractal decomposition of the search space. A \textit{scoring} search component allows to balance the exploration and exploitation of the search space by selecting the promising fractals. \\

The {\it exploration} could be done in a passive way (e.g. Markov chain Monte Carlo (MCMC) sampling, low discrepancy sequences) or in an active way (e.g. metaheuristics such as evolutionary algorithms and swarm intelligence, surrogate-based optimization). The challenge here is to find efficient sampling algorithms for diverse and complex fractals such as polytopes. When a fractal reaches the maximum depth of the partition tree, an {\it exploitation} algorithm can be applied to it. The exploitation phase has not to be constrained within a fractal. The only bounds will be ones from the initial search space so that the exploitation can move freely toward a local or global optimum. One can use local search strategies such as gradient-based algorithms or simulated annealing.

\subsection{Parallel Capabilities}
\label{sec:WP5:Zellij:performances}

Some parallel models of fractal optimization algorithms have been implemented using MPI. In short term perspective, We are currently investigating the use of GPUs to implement some parallel models of fractal-based optimization algorithms using Kokkos parallel environment.

In medium term perspective, hybrid parallel computing will be considered using MPI and Kokkos. The main goal is to target massively parallel clusters of CPU-GPUs.
    
The MPI version has been deployed on multi-nodes and multi-CPUs distributed environments (e.g. Grid'5000). Some experiments were conducted on 66 Intel Xeon Gold 5220 CPUs of 18 cores, with 96 GiB of RAM each. Each CPU is mounted on a Dell PowerEdge R640 rack, and each are on a single node. Nodes are connected on a DELL POWERSWITCH Z9264F-ON, via Ethernet configured at a 25Gbps rate, with SR-IOV activated. \\

Three parallel designs focusing on different levels of a metaheuristic can be designed:
\begin{itemize}
    \item Algorithm-level: Here several metaheuristics are applied in parallel. These can be independent, competitive, or cooperative.
    \item Iteration-level: At each iteration, a set of solutions is generated. In general, this approach consists in parallelizing the evaluation of a set of solutions.
    \item Solution-level: The evaluation of a single solution is parallelized. We are not concerned by this parallelization level, as it depends on the target optimization problem. This parallel level can be relevant in the case where the objective function is expensive in terms of computation time.
\end{itemize}
    
The algorithm level and solution-level are limited in terms of scalability. Using the iteration-level in conjunction with the two other models, one can generate highly scalable optimization algorithms. \\

The efficiency of the parallel models depends also on the cost of the objective function and the dimension of the optimization problem. Less scalability is need for easy optimization problems where the objective function is not expensive and/or the dimen,sion is small. The framework can been integrated with many librairies. For AutoML experiments, it will be integrated with PyTorch and BoTorch.

\subsection{Initial Performance Metrics}
\label{sec:WP5:Zellij:metrics}

\begin{itemize}
\item \textbf{Overall Performance:}

Preliminary computational experiments show the importance of the asynchronous issues for high-dimension and/or stochastic optimization problems. This work also shows the impact of the parallel implementation to the various search components such as tree search, fractal object, exploration, and exploitation. \\

The obtained speedups were sublinear, except for one function of the AutoML benchmark, where we observed a superlinear speedup. Different behaviors were obtained according to the configuration of the search components and the computation time of the objective function. We showed
that the number of processes and their distribution should be carefully selected according to the problem’s dimensionality and/or the computation time of the objective function.

The genericity and flexibility of Zellij, allows the design and the experimentation of new efficient search components for a better scalability to handle exascale optimization. Future experiments on massively parallel architectures will allow analyzing the limitations of the developed master/workers models and hence introduce more distributed parallel models. \\

Another important perspective is the solving of real-life complex optimization problems such as the hyperparameters optimization and the neural architecture search of deep neural networks. Those high-impact optimization problems are characterized by expensive and stochastic objective functions
(i.e., learning procedures) and a huge number of variables (i.e., thousands to millions).

\item \textbf{Input/Output Dataset:} 
\begin{itemize}
\item Input dataset: Optimization problems to solve are defined as functions in a program (e.g. Python or C++): ${f: \mathcal{S} \subset \mathbb{R}^n \rightarrow \mathbb{R}}$, where $\hat{x}$ is the global optima, $f$ the objective function, and $\mathcal{S}$ a compact set made of inequalities (e.g. upper and lower bounds of the search space). 

\item Output dataset: The output is mainly the solutions found (i.e vectors of continuous values) and their quality. Some plots related to convergence can also be obtained.

\item Location of the dataset: In recent years, there have been significant developments in the field of optimization, with new algorithms being proposed to solve challenging problems. As a result, it has become increasingly important to update traditional testing criteria to evaluate the performance of these optimization algorithms. 

Therefore, three main standard librairies are used:
\begin{itemize}
\item CEC2020 benchmark: Single Objective Bound Constrained Numerical Optimization. \\
See \url{https://www.kaggle.com/code/kooaslansefat/cec-2022-benchmark}
\item The blackbox optimization benchmarking (bbob) test suite. \\
See \url{https://numbbo.github.io/coco/testsuites/bbob}
\item HPOBench: A collection of Reproducible Multi-Fidelity Benchmark Problems for hyperparameter optimization of machine learning models. \\
See \url{https://github.com/automl/HPOBench}
\end{itemize}
\end{itemize}

\item \textbf{open-data Access:} All the datasets used are open access. The links are provided in the previous section.

\item \textbf{Challenges:} The main challenges of the benchmarking process are twofold:
\begin{itemize}
\item Scalability of parallel decomposition-based optimization algorithms: it consists in analyzing and improving the obtained speedups of the various parallel models that can be designed.
\item Characteristics of the target optimization problem: it consists in studying the characteristics of the optimization problems (e.g. dimension, cost of the objective function) that affect the performance of the algorithms 
\end{itemize}

\item \textbf{Future Improvements:} The future improvements will concern the following aspects:
\begin{itemize}
\item Parallel models of algorithms: it will be interesting yo used in conjunction the three parallel models of decomposition-based optimization algorithms. Those hybrid models will improve the scalability of the algorithms to target Exascale architectures.

\item Target architectures: in addition to CPU and GPU based architectures, we have to design and experiment the parallel models which are adapted to heterogeneous architectures combining GPU and CPU. Load balancing and faut tolerance aspects have to proposed.

\item Optimization problems: in addition to standard benchmarks of optimization problems, we are interested to real-life complex applications in the area of AutoML such as hyperparameter optimization (HPO) and neural architecture search (NAS).

The AutoDNN problem is characterized by the following important properties: Large-scale optimization problem, Mixed optimization problem, Variable-size design space, expensive black-box objective function(s), and Multi-objective optimization problem.

\end{itemize}

\end{itemize}

\subsubsection{Benchmark \#1}
\begin{itemize}
\item \textbf{Description:} The first family of benchmarks consists in well-known benchmarks in global optimization community, such as CEC2020, SOCO2011 and BBOB.

\item \textbf{Benchmarking Tools Used:} 

Those benchmarks are made of more than 24 functions with peculiar properties such as, separability, ill-conditioning, multi-modality and weak structured multi-modality. Each function has many different instances, and are available for many dimension sizes, $d \in  \{ 2,3,5,10,20,40, ... \}$. 

The following metrics have been used in our experiments:  measured are: execution time, budget, quality of solutions, speedup).

\item \textbf{Input/Output Dataset Description:}
\begin{itemize}

\item \textbf{Input Data:} the input data describes the optimization problem (e.g. continuous function) to solve and the associated data. 

\item \textbf{Output Data:} the results details the best solution found for solving the problem. 

\item \textbf{Results Summary:} solving academic optimization problems (i.e. non expensive objective function), even high dimension problems, is not an issue in terms of execution time and other resource usage. However, solving AutoML optimization problems (e.g. neural architecture search of deep neural networks, hyper parameter optimization of large langage models - LLM) takes a lot of time (e.g. some days). Experiments needs the access to at least Petaflop architectures.

\item \textbf{Challenges Identified:} the main challenges is the scalability of the designed parallel models. Moreover, one has to design some load balancing and fault tolerance strategies to support large scale heterogeneous parallel architectures. 


\item \textbf{Benchmarking Tools Used:} 

Those benchmarks are made of more than 24 functions with peculiar properties such as, separability, ill-conditioning, multi-modality and weak structured multi-modality. Each function has many different instances, and are available for many dimension sizes, $d \in  \{ 2,3,5,10,20,40, ... \}$. 
\end{itemize}

The following metrics have been used in our experiments: execution time, budget, quality of solutions, speedup). Converengce plots are also provided to compare the behavior of different optimization algorithms.

%\begin{itemize}

%\item \textbf{Input/Output Dataset Description:}

%\item \textbf{Input Data:} Describe the input dataset (size, format, data type) and provide a DOI or link to access it.

%\item \textbf{Output Data:} Specify the structure of the results (e.g., memory usage, runtime logs) and how they can be accessed or replicated.

%\item \textbf{Data Repository:} Indicate where the data is stored (e.g., Zenodo, institutional repository) and provide a DOI or URL for accessing the data.

%\item \textbf{Results Summary:} Include a summary of key metrics (execution time, memory usage, FLOPS) and their comparison across architectures (e.g., CPU, GPU).

%\item \textbf{Challenges Identified:} Describe any bottlenecks encountered (e.g., memory usage, parallelization inefficiencies) and how they impacted the benchmark.
%\end{itemize}


\subsubsection{Benchmark \#2}

The second family of benchmarks consists in well-known benchmarks from machine learning community (AutoML). 

The main characteristics of those problems in their black box and expensive objective functions.

Automated Machine Learning (AutoML) consists in automating the application of machine-learning models to realworld applications. The goal of this approach is to design better machine learning models. Because, we are interested in applying this methodology to the HyperParameter Optimization
(HPO) of expensive deep neural networks, we have selected three benchmarks from HPOBench. We
performed a HPO on 3 functions:
\begin{itemize}
\item Random Forest (RF): 4 hyperparameters (n = 4) are optimized, with a budget of 20000 calls.
\item Multi-layer Neural Network (NN): 5 hyperparameters (n = 5) are optimized, with a budget of 6000 calls.
\item Histogram-Based Gradient Boosting (HBGB): 4 hyperparameters (n = 4) are optimized, with a budget of 6000 calls.
\end{itemize}

The budget was defined according to their computation time. These models were trained and tested on a given task of OpenML (supervised classification on car). We have selected these three models for their simplicity. They do not require any GPU, and the computation time is low compared to expensive DNN benchmarks, but higher compared to low-cost academic functions usually used to
validate performances of continuous optimization algorithms.

The following metrics have been used in our experiments:  measured are: execution time, budget, quality of ML models, speedup, convergence plots).

\begin{itemize}
\item \textbf{Input/Output Dataset Description:} The HPBbench is available under the URL: \url{https://www.automl.org/hpo-overview/hpo-benchmarks/hpobench/}
\end{itemize}

HPOBench aims at solving the aforementioned issues with the following contributions:

\begin{itemize}
\item A standard API that allows the users to evaluate multi-fidelity HPO on several benchmarks and their corresponding fidelities.
\item Containers that isolate the benchmarks from the computation environments and mitigate the problem of software dependencies.
\item Surrogate and Tabular benchmarks provide a cheap way of evaluating the target algorithms.
\end{itemize}

HPOBench currently contains more than 100 multi-fidelity benchmark problems with various properties: numerical and categorical configuration space, different difficulties, and complexities. Furthermore, HPOBench also provides the result of several popular HPO packages to make them easier to be compared with the new HPO algorithms. For more information, please check our HPOBench GitHub repository.

%\begin{itemize}
%\item \textbf{Output Data:} Specify the structure of the results (e.g., memory usage, runtime logs) and how they can be accessed or replicated.

%\item \textbf{Data Repository:} Indicate where the data is stored (e.g., Zenodo, institutional repository) and provide a DOI or URL for accessing the data.

%\item \textbf{Results Summary:} Include a summary of key metrics (execution time, memory usage, FLOPS) and their comparison across architectures (e.g., CPU, GPU).

%\item \textbf{Challenges Identified:} Describe any bottlenecks encountered (e.g., memory usage, parallelization inefficiencies) and how they impacted the benchmark.
\end{itemize}

\subsection{12-Month Roadmap}
\label{sec:WP5:Zellij:roadmap}

\begin{itemize}

\item \textbf{Data Improvements:} There are no issues concerning the improvements of input/output data management. All datasets are accessible and reproducibility is ensured through open-data initiatives.

\item \textbf{Methodology Application and result retention :} All obtained results are open access and are published in HAL, conferences and journals.
\end{itemize}

In~\cref{tab:WP5:Zellij:bottlenecks}, we briefly discuss the bottleneck roadmap associated to the software and relevant to the work package.

\begin{table}[h!]
    \centering
    
    \centering
    { 
        \setlength{\parindent}{0pt}
        \def\arraystretch{1.25}
        \arrayrulecolor{numpexgray}
        {
            \fontsize{9}{11}\selectfont
            \begin{tabular}{!{\color{numpexgray}\vrule}p{.25\linewidth}!{\color{numpexgray}\vrule}p{.6885\linewidth}!{\color{numpexgray}\vrule}}
    
    \rowcolor{numpexgray}{\rule{0pt}{2.5ex}\color{white}\bf Bottlenecks} &  {\rule{0pt}{2.5ex}\color{white}\bf Short Description }\\ 
    
\rowcolor{numpexlightergray}    B7 - Exascale Algorithms & We are investigating currently the parallel implementation of the algorithms on hybrid inter-node architectures combining GPU and CPU using the environments MPI and Kokkos. In addition to Grid'5000 platform we will target some larger scale machines such as Jean Zay. \\

\rowcolor{white}    B9 - Resilience, robustness and accuracy & Some load balancing and fault tolerance strategies have to be designed for large Exascale heterogeneous architectures. \\
 
 \rowcolor{numpexlightergray}    B10 - Scientific Productivity & The obtained results in terms of parallel algorithm design and implementation and their application to big optimization problems will be published in conferences and journals in the field of optimization and parallel computing. \\
\end{tabular}
        }
    }
    \caption{WP5: Zellij plan with Respect to Relevant Bottlenecks}
    \label{tab:WP5:Zellij:bottlenecks}
\end{table}