%!TEX root = ../../../exa-ma-d7.1.tex
\section{Software: \Feelpp}
\label{sec:WP1:Feelpp:software}

\begin{table}[h!]
    \centering
    { \setlength{\parindent}{0pt}
    \def\arraystretch{1.25}
    \arrayrulecolor{numpexgray}
    {\fontsize{9}{11}\selectfont
    \begin{tabular}{!{\color{numpexgray}\vrule}p{.4\textwidth}!{\color{numpexgray}\vrule}p{.6\textwidth}!{\color{numpexgray}\vrule}}
        \rowcolor{numpexgray}{\rule{0pt}{2.5ex}\color{white}\bf Field} & {\rule{0pt}{2.5ex}\color{white}\bf Details} \\
        \rowcolor{white}\textbf{Consortium} & \begin{tabular}{l}
\Feelpp Consortium\\
\end{tabular} \\
        \rowcolor{numpexlightergray}\textbf{Exa-MA Partners} & \begin{tabular}{l}
CNRS\\
Inria Grenoble\\
Unistra\\
\end{tabular} \\
        \rowcolor{white}\textbf{Contact Emails} & \begin{tabular}{l}
christophe.prudhomme@cemosis.fr\\
vincent.chabannes@cemosis.fr\\
\end{tabular} \\
        \rowcolor{numpexlightergray}\textbf{Supported Architectures} & \begin{tabular}{l}
CPU Only\\
\end{tabular} \\
        \rowcolor{white}\textbf{Repository} & \href{https://github.com/feelpp/feelpp}{https://github.com/feelpp/feelpp} \\
        \rowcolor{numpexlightergray}\textbf{License} & \begin{tabular}{l}
OSS:: GPL v*\\
OSS:: LGPL v*\\
\end{tabular} \\
        \rowcolor{white}\textbf{Bottlenecks roadmap} & \begin{tabular}{l}
B10 - Scientific Productivity\\
B11 - Reproducibility and Replicability of Computation\\
B12 - Pre/Post Processing and In-Situ Processing\\
B2 - Interconnect Technology\\
B6 - Data Management\\
B7 - Exascale Algorithms\\
\end{tabular} \\
        \bottomrule
    \end{tabular}
    }}
    \caption{WP1: \Feelpp Information}
\end{table}

\subsection{Software Overview}
\label{sec:WP1:Feelpp:summary}

In~\cref{tab:WP1:Feelpp:features} we provide a summary of the software features relevant to the work package which are briefly discussed.

\begin{table}[h!]
    \centering
    { 
        \setlength{\parindent}{0pt}
        \def\arraystretch{1.25}
        \arrayrulecolor{numpexgray}
        {
            \fontsize{9}{11}\selectfont
            \begin{tabular}{!{\color{numpexgray}\vrule}p{.25\linewidth}!{\color{numpexgray}\vrule}p{.6885\linewidth}!{\color{numpexgray}\vrule}}
    
    \rowcolor{numpexgray}{\rule{0pt}{2.5ex}\color{white}\bf Features} &  {\rule{0pt}{2.5ex}\color{white}\bf Short Description }\\ 
    
\rowcolor{white}    cG & provide short description here \\
\rowcolor{numpexlightergray}    dG/hdG & provide short description here \\
\rowcolor{white}    finite element & provide short description here \\
\rowcolor{numpexlightergray}    inhouse & provide short description here \\
\rowcolor{white}    interface & provide short description here \\
\rowcolor{numpexlightergray}    mesh adaptation & provide short description here \\
\rowcolor{white}    multiphysics coupling & provide short description here \\
\rowcolor{numpexlightergray}    multiscale coupling & provide short description here \\
\rowcolor{white}    parallel in time & provide short description here \\
\rowcolor{numpexlightergray}    spectral element & provide short description here \\
\rowcolor{white}    unstructured mesh & provide short description here \\
\end{tabular}
        }
    }
    \caption{WP1: \Feelpp Features}
    \label{tab:WP1:Feelpp:features}
\end{table}


\subsection{Parallel Capabilities}
\label{sec:WP1:Feelpp:performances}


\begin{itemize}
    \item describe the parallel programming  environment : MPI, OpenMP, CUDA, OpenACC, etc.
    \item describe the parallel computation environment: type of architecture and super computer used.
    \item describe the parallel capabilities of the software
    \item \textbf{Scalability:} Describe the general scalability properties of the software
    \item \textbf{Integration with Other Systems:} Describe how the software integrates with other numerical libraries in the Exa-MA framework.
\end{itemize}


\subsection{Initial Performance Metrics}
\label{sec:WP1:Feelpp:metrics}

This section provides a summary of initial performance benchmarks performed in the context of WP1. It ensures reproducibility by detailing input/output datasets, benchmarking tools, and the results. All data should be publicly available, ideally with a DOI for future reference.

\begin{itemize}
    \item \textbf{Overall Performance:} Summarize the software's computational performance, energy efficiency, and scalability results across different architectures (e.g., CPU, GPU, hybrid systems).
    \item \textbf{Input/Output Dataset:} Provide a detailed description of the dataset used for the benchmark, including:
        \begin{itemize}
            \item Input dataset size, structure, and format (e.g., CSV, HDF5, NetCDF).
            \item Output dataset format and key results.
            \item Location of the dataset (e.g., GitHub repository, institutional repository, or open access platform).
            \item DOI or permanent link for accessing the dataset.
        \end{itemize}
    \item \textbf{open-data Access:} Indicate whether the datasets used for the benchmark are open access, and provide a DOI or a direct link for download. Where applicable, highlight any licensing constraints.
    \item \textbf{Challenges:} Identify any significant bottlenecks or challenges observed during the benchmarking process, including data handling and computational performance.
    \item \textbf{Future Improvements:} Outline areas for optimization, including dataset handling, memory usage, or algorithmic efficiency, to address identified challenges.
\end{itemize}

\subsubsection{Benchmark \#1: Compute Distance Function}

\paragraph{Description}
This benchmark evaluates two methods for computing the distance function inside a three-dimensional box: 
\begin{enumerate}
    \item The \textbf{Level Set} method using the \textbf{Fast Marching Algorithm (FMA)}.
    \item The \textbf{Ray Tracing} method.
\end{enumerate}
The objective is to compute the distance function at all vertices of a discretized box using both methods and verify whether they produce the same results. 
The problem is discretized using an unstructured grid, and performance is assessed on a multi-core CPU architecture.

The benchmark aims to compare the efficiency, accuracy, and computational cost of both approaches in terms of distance calculation within the 3D domain.

\paragraph{Benchmarking Tools Used}
The following tools were used for performance profiling and analysis:
\begin{itemize}
\item \textbf{\Feelpp}: the performance tools integrated into the \Feelpp framework were used to measure the execution time.
\end{itemize}

The key metrics measured include execution time, accuracy, memory usage, and floating-point operations (FLOPS) for both methods.

\subsection{Input/Output Dataset Description}
\begin{itemize}
    \item \textbf{Input Data:} The input consists of a 3D uniform grid representing the box geometry, with approximately 1 million vertices. The level set function and ray tracing boundaries are initialized for the distance computation. The input data is stored in JSON format, and it can be accessed via DOI: \texttt{[Insert DOI]}.
    
    \item \textbf{Output Data:} The output includes the computed distance function values at all vertices for both methods, stored in CSV format. Additionally, runtime performance logs and accuracy comparisons between the methods are included.
    
    \item \textbf{Data Repository:} Input and output datasets, along with performance logs, are stored in a Zenodo repository and can be accessed via DOI: \texttt{[Insert DOI]}.
\end{itemize}

\paragraph{Results Summary}
The performance comparison between the two methods is summarized as follows:

RESULTS here.

\paragraph{Challenges Identified}
The following challenges were encountered during the benchmarking process:
\begin{itemize}
    \item \textbf{Ray Tracing Bottlenecks:} 
    \item \textbf{Parallelization Issues:} 
    \item \textbf{Memory Usage:} 
\end{itemize}

Final analysis and persectives here.

\begin{itemize}
    \item \textbf{Description:} Briefly describe the benchmark case, including the problem size, target architecture (e.g., CPU, GPU), and the input data. Mention the specific goals of the benchmark (e.g., testing scalability, energy efficiency).
    \item \textbf{Benchmarking Tools Used:} List the tools used for performance analysis, such as Extrae, Score-P, TAU, Vampir, or Nsight, and specify what metrics were measured (e.g., execution time, FLOPS, energy consumption).
    \item \textbf{Input/Output Dataset Description:}
        \begin{itemize}
            \item \textbf{Input Data:} Describe the input dataset (size, format, data type) and provide a DOI or link to access it.
            \item \textbf{Output Data:} Specify the structure of the results (e.g., memory usage, runtime logs) and how they can be accessed or replicated.
            \item \textbf{Data Repository:} Indicate where the data is stored (e.g., Zenodo, institutional repository) and provide a DOI or URL for accessing the data.
        \end{itemize}
    \item \textbf{Results Summary:} Include a summary of key metrics (execution time, memory usage, FLOPS) and their comparison across architectures (e.g., CPU, GPU).
    \item \textbf{Challenges Identified:} Describe any bottlenecks encountered (e.g., memory usage, parallelization inefficiencies) and how they impacted the benchmark.
\end{itemize}

\subsubsection{Benchmark \#2: Assemble Stiffness and Linear Elasticity Matrix}

\paragraph{Description}
This benchmark evaluates the assembly of stiffness and linear elasticity finite element matrices in three dimensions using both continuous Galerkin (cG) and hybrid discontinuous Galerkin (hdG) methods using the \Feelpp toolboxes.
The problem size consists of a mesh with tetrahedral elements, executed entirely on multi-core CPU architectures.
The benchmark is designed to measure scalability, execution time, and computational efficiency across different material models, including isotropic and anisotropic materials.

The objective of the benchmark is to assess performance in terms of assembly time, memory usage, and parallel efficiency for cG and hdG methods from low to high orders using CPU resources only.

\paragraph{Benchmarking Tools Used}
The following performance analysis tools were used:
\begin{itemize}
    \item \textbf{\Feelpp}: the performance tools integrated into the \Feelpp framework were used to measure the execution time and memory usage during the matrix assembly.
\end{itemize}

Metrics such as execution time, memory usage, and FLOPS were measured to compare the performance of the cG and hdG methods on CPU.

\paragraph{Input/Output Dataset Description}
\begin{itemize}
    \item \textbf{Input Data:} The input dataset consists of a 3D tetrahedral mesh generated using the Gmsh format, with approximately 1 million elements. Material properties are defined in JSON format, covering both isotropic and anisotropic materials. 
    
    \item \textbf{Output Data:} The output includes performance logs, execution times, and memory usage reports. Output results are replicable by using the same mesh and material properties. 
    
    \item \textbf{Data Repository:} All input and output datasets are available in a Zenodo repository, accessible through DOI: \texttt{[Insert DOI]}.
\end{itemize}

\paragraph{Results Summary}
The benchmark results are summarized as follows:

RESULTS here

The results highlight that ... (ADD ANALYSIS)

\paragraph{Challenges Identified}
Several challenges were encountered during the benchmarking process:
\begin{itemize}
    \item \textbf{Memory Usage:} 
    \item \textbf{Parallelization Inefficiencies:} 
    \item \textbf{Cache and Memory Bottlenecks:} 
\end{itemize}

add extra analysis  and conclusion here.

\subsubsection{Benchmark \#3: Thermo-Electric Coupling}

Thermo Electric coupling in a complex geometry.

\subsubsection{Benchmark \#4: HeatFluid Coupling}

Steady Conjuguate Heat Transfer in a complex geometry. d 


\subsubsection{Benchmark \#5: Contact Mechanics}

\begin{itemize}
    \item combine ray tracing, assembly, contact mechanics, time dependent 
\end{itemize}

\subsection{12-Month Roadmap}
\label{sec:WP1:Feelpp:roadmap}

In this section, describe the roadmap for improving benchmarks and addressing the challenges identified. This should include:
\begin{itemize}
    \item \textbf{Data Improvements:} Plans for improving input/output data management, including making datasets more accessible and ensuring reproducibility through open-data initiatives.
    \item \textbf{Methodology Application:} Implementation of the benchmarking methodology proposed in this deliverable to streamline reproducibility and dataset management.
    \item \textbf{Results Retention:} Plans to maintain benchmark results in a publicly accessible repository with appropriate metadata and documentation, ensuring long-term usability.
\end{itemize}

In~\cref{tab:WP1:Feelpp:bottlenecks}, we briefly discuss the bottleneck roadmap associated to the software and relevant to the work package.

\begin{table}[h!]
    \centering
    
    

    \centering
    { 
        \setlength{\parindent}{0pt}
        \def\arraystretch{1.25}
        \arrayrulecolor{numpexgray}
        {
            \fontsize{9}{11}\selectfont
            \begin{tabular}{!{\color{numpexgray}\vrule}p{.25\linewidth}!{\color{numpexgray}\vrule}p{.6885\linewidth}!{\color{numpexgray}\vrule}}
    
    \rowcolor{numpexgray}{\rule{0pt}{2.5ex}\color{white}\bf Bottlenecks} &  {\rule{0pt}{2.5ex}\color{white}\bf Short Description }\\ 
    
\rowcolor{white}    B10 - Scientific Productivity & provide short description here \\
\rowcolor{numpexlightergray}    B11 - Reproducibility and Replicability of Computation & provide short description here \\
\rowcolor{white}    B12 - Pre/Post Processing and In-Situ Processing & provide short description here \\
\rowcolor{numpexlightergray}    B2 - Interconnect Technology & provide short description here \\
\rowcolor{white}    B6 - Data Management & provide short description here \\
\rowcolor{numpexlightergray}    B7 - Exascale Algorithms & provide short description here \\
\end{tabular}
        }
    }
    \caption{WP1: \Feelpp plan with Respect to Relevant Bottlenecks}
    \label{tab:WP1:Feelpp:bottlenecks}
\end{table}