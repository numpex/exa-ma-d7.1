%!TEX root = ../exa-ma-d7.1.tex
\clearpage
\chapter{Software Frameworks powering Applications}
\label{chap:software}

This chapter presents the Exa-MA \emph{software frameworks} that power our applications—mini‑apps, extended mini‑apps, demonstrators, and proxy‑apps—using the terminology defined in Chapter~\ref{chap:methodology} (Structured Application Definitions). Each framework section briefly explains: (i) what the framework provides, (ii) which Exa‑MA Work Packages benefit, and (iii) which kinds of applications it enables in Chapter~\ref{chap:applications}.

We first provide general statistics across frameworks (architectures, languages, parallelism, I/O, DevOps). Then, we list the frameworks with concise capability summaries and pointers to their application entry points.

In the context of D7.1, we also clarify architectural capabilities (CPU/GPU/hybrid) to align with the benchmarking and CI/CD pipelines.

\paragraph{Application taxonomy (from methodology).}
Following Chapter~\ref{chap:methodology}:
\begin{description}
    \item[Mini‑app] Single kernel/algorithm focus (e.g., a preconditioner, mesh tool).
    \item[Extended mini‑app] Mini‑app with auxiliary scripts/data and I/O benchmarks.
    \item[Demonstrator] Integrated multi‑WP workflow for a scientific use‑case.
    \item[Proxy‑app] Representative full‑stack workload ($\geq 3$ WPs) for system tests.
\end{description}

\paragraph{Framework→Application links.}
For each framework below, we reference examples in Chapter~\ref{chap:applications} when available (e.g., Feel++ reduced‑basis mini‑apps, HPDDM‑enabled solver demonstrators, FreeFEM++ mesh‑adaptation extended mini‑apps).

The architectural capabilities benchmarked are classified as follows:

\paragraph{CPU Only}
Framework executes exclusively on CPUs (e.g., CGAL, FreeFEM++, Feel++, Manta). Benchmarks target CPU nodes.

\paragraph{GPU Only}
Framework executes exclusively on GPUs (e.g., Zellij). Benchmarks target GPU nodes.

\paragraph{CPU or GPU}
Framework can run on either CPU or GPU, but not concurrently within one run (e.g., PyTorch, SciMba). We benchmark both modes separately.

\paragraph{CPU and GPU}
Framework supports simultaneous CPU+GPU execution within a single run (e.g., TRUST). Benchmarks verify mixed‑architecture utilization.

\paragraph{Explanation of Benchmarking Criteria}
\begin{itemize}
    \item \textbf{CPU Only:} If selected, benchmarks will be performed exclusively on CPU architectures.
    \item \textbf{GPU Only:} If selected, benchmarks will focus exclusively on GPU architectures.
    \item \textbf{CPU and GPU:} If selected, benchmarks will involve simultaneous execution on CPU and GPU within the same run.
    \item \textbf{CPU or GPU:} If selected, benchmarks will involve execution on either CPU or GPU, but not simultaneously. Some benchmarks will target CPU, others GPU, depending on the computational components.
\end{itemize}


\section{General Statistics across Frameworks}
\label{sec:software:statistics}

In this section, we provide an overview of the key characteristics and technological choices for the software developed and benchmarked within Exa-MA. 
These statistics offer insights into the diversity of hardware architectures, programming languages, and parallel computing technologies utilized across the different software packages. 

The aim is to highlight the widespread usage of various technologies, demonstrating both the flexibility and the breadth of approaches within the project. 
Additionally, the DevOps practices employed, such as continuous integration, testing, and deployment, are presented to underscore the commitment to ensuring quality, reliability, and maintainability of the software developed under Exa-MA.

The following subsections show different aspects of the software frameworks involved in the project—supported architectures and programming languages, parallelism technologies, data formats, and DevOps strategies. This helps assess readiness for large‑scale simulations, CI/CD integration, and demonstrator pipelines.


\subsection{Architectures}

The following pie chart~\ref{fig:arch} shows the distribution of hardware architectures used for the benchmarks.

\begin{figure}[H]
\centering
\begin{tikzpicture}
\pie[text=legend, color={red, orange, yellow, lime}, sum=auto]{10/CPU Only, 5/CPU or GPU, 1/CPU and GPU, 2/GPU Only}
\end{tikzpicture}
\caption{Distribution of benchmarked hardware architectures}
\label{fig:arch}
\end{figure}


\subsection{Programming Languages}

The following pie chart~\ref{fig:languages} shows the distribution of programming languages used, highlighting the variety of computational solutions employed.

\begin{figure}[H]
\centering
\begin{tikzpicture}
\pie[text=legend, color={red, orange, yellow, lime, skyblue, pink, cyan, magenta}, sum=auto]{12/C++, 1/C\#, 2/C, 3/Fortran, 2/C++17, 5/Python, 1/C++20, 1/C++14}
\end{tikzpicture}
\caption{Distribution of programming languages}
\label{fig:languages}
\end{figure}


\subsection{Parallelism Technology}


The pie chart~\ref{fig:parallelism} below represents the parallelism techniques used in Exa-MA software selected for this document.

\begin{figure}[H]
\centering
\begin{tikzpicture}
\pie[text=legend, color={red, orange, yellow, lime, skyblue, pink}, sum=auto]{8/Multithread, 13/MPI, 8/GPU, 1/Parallelism - C++, 1/Task based, 1/Chapel}
\end{tikzpicture}
\caption{Distribution of parallelism technologies}
\label{fig:parallelism}
\end{figure}



\subsection{Data Formats}

The chart~\ref{fig:data} shows the supported data formats, for flexibility and compatibility in data handling, supported by Exa-MA software selected for this document.

\begin{figure}[H]
\centering
\begin{tikzpicture}
\pie[text=legend, color={red, orange, yellow, lime, skyblue, pink, cyan, magenta, peach, lavender}, sum=auto]{2/XML, 5/HDF5, 3/JSON, 2/Ensight, 5/None, 1/YAML, 1/Data-management system, 6/VTK, 5/in-house format, 4/Gmsh and associated formats, 2/MED, 1/MFront, 1/ASCII, 1/ROOT, 1/SQL}
\end{tikzpicture}
\caption{Distribution of data formats}
\label{fig:data}
\end{figure}


\subsection{Resilience}

The pie chart~\ref{fig:resilience} below shows the resilience mechanisms used in Exa-MA software selected for this document.

\begin{figure}[H]
\centering
\begin{tikzpicture}
\pie[text=legend, color={red}, sum=auto]{6/Checkpoint restart}
\end{tikzpicture}
\caption{Distribution of Resilience strategies}
\label{fig:resilience}
\end{figure}



\subsection{DevOps - CI/CD}

The pie chart~\ref{fig:devops-cicd} below displays the support of continuous integration and deployment practices as well as continuous benchmarking, showcasing systematic software updates, quality maintenance and performance regression.

\begin{figure}[H]
\centering
\begin{tikzpicture}
\pie[text=legend, color={red, orange, yellow, lime}, sum=auto]{11/Continuous Integration, 1/Continuous Benchmarking, 2/Continuous Delivery, 4/None}
\end{tikzpicture}
\caption{Distribution of DevOps CI/CD/CD}
\label{fig:devops-cicd}
\end{figure}


\subsection{DevOps - Packaging}

The next chart~\ref{fig:devops-packaging} shows different packaging methods used, which help in the distribution and management of software.

\begin{figure}[H]
\centering
\begin{tikzpicture}
\pie[text=legend, color={red, orange, yellow, lime, skyblue, pink}, sum=auto]{10/None, 5/Debian-based, 2/Fedora, 2/Spack, 1/GUIX, 1/Other}
\end{tikzpicture}
\caption{Distribution of DevOps Packaging}
\label{fig:devops-packaging}
\end{figure}


\subsection{DevOps - Containers}

The pie chart~\ref{fig:devops-containers} displays the use of container technologies, which help encapsulate the software to run reliably in various environments.

\begin{figure}[H]
\centering
\begin{tikzpicture}
\pie[text=legend, color={red, orange, yellow}, sum=auto]{12/None, 2/Apptainer/Singularity, 2/Docker}
\end{tikzpicture}
\caption{Distribution of DevOps Containers}
\label{fig:devops-containers}
\end{figure}


\subsection{DevOps - Testing}

The following pie chart~\ref{fig:devops-testing} details the testing practices adopted, illustrating the commitment to software reliability and functionality.

\begin{figure}[H]
\centering
\begin{tikzpicture}
\pie[text=legend, color={red, orange, yellow, lime, skyblue}, sum=auto]{9/Unit, 8/Verification, 4/Validation, 5/None, 1/Functional}
\end{tikzpicture}
\caption{Distribution of DevOps Testing}
\label{fig:devops-testing}
\end{figure}


% Framework descriptions (each subfile may also mention example mini‑apps/demonstrators)
%\input{software/arcane-framework/arcane-framework.tex}
\input{software/cgal/cgal.tex}
\input{software/composyx/composyx.tex}
\input{software/feelpp/feelpp.tex}
\input{software/freefempp/freefempp.tex}
\input{software/hawen/hawen.tex}
\input{software/hpddm/hpddm.tex}
%%\input{software/mahyco/mahyco.tex}
\input{software/manta/manta.tex}
\input{software/pbb/pbb.tex}
\input{software/samurai/samurai.tex}
\input{software/scimba/scimba.tex}
\input{software/trust-platform/trust-platform.tex}
\input{software/uranie/uranie.tex}
\input{software/zellij/zellij.tex}

\section{Frameworks to Applications Mapping (Overview)}
This section summarizes how frameworks are used to construct applications as per Chapter~\ref{chap:methodology} and Chapter~\ref{chap:applications}. Detailed specifications remain in Chapter~\ref{chap:applications}.

\begin{itemize}
    \item \textbf{Feel++} → mini‑apps (reduced basis, I/O), demonstrators (thermal, CFD); see Sections on app‑feelpp‑rb, app‑feelpp‑io.
    \item \textbf{HPDDM} → solver kernels for demonstrators and proxy‑apps (domain decomposition preconditioning).
    \item \textbf{FreeFEM++} → extended mini‑apps (mesh adaptation with ParMMG), rapid inverse‑problem templates.
    \item \textbf{CGAL} → geometry/meshing utilities in mini‑apps (wrapping, AABB trees).
    \item \textbf{SciMba/PyTorch} → surrogates for WP2 integrated into demonstrators (multi‑fidelity modeling).
    \item \textbf{TRUST} → CPU+GPU demonstrators.
    \item \textbf{Uranie} → UQ workflows supporting demonstrators and proxy‑apps.
    \item \textbf{Zellij} → optimization engines inside WP5 demonstrators.
\end{itemize}

This mapping is non‑exhaustive and will expand as additional applications mature.

