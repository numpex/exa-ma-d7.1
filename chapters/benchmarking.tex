%!TEX root = ../exa-ma-d7.1.tex

\chapter{Benchmarking Methodology}
\label{chap:methodology}

\section{Introduction}
\label{sec:methodology-intro}


This chapter outlines a methodology for benchmarking, testing, and validating the software developed within the \exama project. 
As we move towards exascale computing, several bottlenecks (from \ac{B1} to \ac{B13}) must be addressed, including \ac{B1}, \ac{B2}, \ac{B3}, \ac{B6}, \ac{B7} and \ac{B11}. 
The methodology proposed in this chapter addresses these challenges with a focus on performance, scalability, energy efficiency, and reproducibility. 

Indeed \ac{exama} focuses not only on ensuring exascale performance but also on establishing reproducibility through open-data, permanent links (DOIs), and long-term results retention in public repositories such as Zenodo. 

The methodology integrates several core components:
\begin{itemize}
    \item \textbf{Testing and Validation:} A structured process for non-regression testing, verification, and validation, ensuring that new software iterations do not degrade performance or introduce errors, while validating enhancements in real-world scenarios.
    \item \textbf{Benchmarking Strategy:} A phased benchmarking approach designed to measure the performance, scalability, and energy efficiency of the software, from baseline performance to optimized scalability testing.
    \item \textbf{Data Management and Reproducibility:} Incorporation of input/output dataset descriptions, open-data access with DOIs, and proper documentation of datasets in repositories like Zenodo to ensure reproducibility.
    \item \textbf{Profiling and Measurement:} The use of advanced profiling and performance measurement tools such as Extrae, Score-P, TAU, Vampir, and Nsight to gather detailed insights into software performance across different architectures.
    \item \textbf{Containerization and Packaging:} The use of packaging and containerization technologies such as Spack, Guix-HPC, Docker, and Apptainer/Singularity enables reproducible and portable execution environments across various hardware platforms, simplifying deployment and reproducibility of the benchmarks.
    \item \textbf{Continuous Integration and Deployment (CI/CD):} The integration of continuous benchmarking and regression testing into a CI/CD pipeline ensures reproducibility, portability, and sustained high-performance across different HPC systems.
\end{itemize}

Through this methodology proposal, the \exama project aims to ensure that its software is exascale-ready, while establishing strong foundations for long-term data management, ensuring performance metrics and results are accessible, reusable, and traceable for future research.




\section{Methodology}
\label{sec:bench:methodology}


\subsection{Key Objectives}
\label{sec:key-objectives}

The objectives of the Work Package 7 are designed to 
\begin{itemize}
    \item \textbf{Developing Software:} Ensure that the software developed in the \exama project is optimized for exascale environments, from simple to advanced cases.
    \item \textbf{Testing and Benchmarking:} Implement benchmarking processes to verify that the software meets exascale capabilities and tackles identified computational challenges.
    \item \textbf{Delivering Software Packages:} Deliver software in a standardized and reproducible form, integrating CI/CD/CB practices as proposed by \exadi.
    \item \textbf{Coordinating Co-Design:} Collaborate with \exadi and across \exama work packages to integrate co-design efforts for efficient development.
    \item \textbf{Creating a Showroom:} Establish a showroom of \exama results, presenting real-world achievements and use cases.
    \item \textbf{Building Training Material:} Develop educational resources based on the results and methodologies adopted in the \exama project, promoting knowledge transfer.
\end{itemize}
Setting up and executing an efficient benchmarking and testing methdology is an important part of the activity and is a critical enabler for \ac{B10}.
The key objectives associated with the corresponding bottlenecks are:
\begin{itemize}
    \item \textbf{Performance and Scalability (\ac{B2},  \ac{B7}):} Evaluate how well software scales and performs across different architectures, from CPUs to GPUs and hybrid environments.
    \item \textbf{Energy Efficiency (\ac{B1}):} Measure energy consumption and optimize for energy-efficient performance, which is critical in exascale computing.
    \item \textbf{Reproducibility and Portability (\ac{B6}, \ac{B11}):} Ensure that benchmarks and software executions can be reproduced and run across different HPC systems.
    \item \textbf{Non-Regression (\ac{B1},\ac{B2},\ac{B6},\ac{B7},\ac{B9},\ac{B10},\ac{B11}):} Guarantee that future software iterations do not regress in performance or accuracy.
    \item \textbf{Validation and Verification (\ac{B1},\ac{B2},\ac{B6},\ac{B7},\ac{B8},\ac{B9},\ac{B10},\ac{B11},\ac{B12},\ac{B13}):} Verify that software behaves correctly and validate its performance in real-world use cases.
\end{itemize}



\subsection{Testing Processes and Non-Regression Strategies}
\label{sec:testing-processes}

In the context of exascale computing, \exama needs to ensure that new developments do not degrade the performance or functionality of existing software.
To this end, the \exama project follows three main processes:

\begin{itemize}
    \item \textbf{Non-Regression Testing:} Ensures that new software versions do not regress in performance or accuracy compared to previous versions.
    \item \textbf{Verification:} Confirms that optimizations or algorithmic improvements do not lead to unacceptable deviations from known solutions.
    \item \textbf{Validation:} Validates that new methods or algorithms proposed within \exama deliver meaningful improvements in real-world, production-level scenarios.
\end{itemize}


\subsubsection{Non-Regression Testing}
\label{sec:non-regression}

The goal of non-regression testing is to guarantee that the results produced by a specific version of the software are identical to those of previous versions, at least for functionalities that should remain unaffected by new developments.

\paragraph{Process:}
\begin{itemize}
    \item Ensure identical results on unchanged code paths or configurations.
    \item Perform these tests on simple cases that involve a limited subset of the software sources, referred to as elementary tests.
    \item Maintain consistency in the outcomes, ensuring that simple functionalities behave as expected across updates unless justified otherwise.
\end{itemize}

\subsubsection{Verification}
\label{sec:verification}

Verification tests aim to confirm that optimizations or algorithmic improvements do not lead to unacceptable deviations from known solutions.

\paragraph{Process:}
\begin{itemize}
    \item Measure the absence of drift in consolidated solutions, especially after performance optimizations.
    \item Apply elementary or level 1 integrated tests, involving few additional software sources.
    \item Introduce an acceptance criterion based on relative deviations in results.
\end{itemize}

\subsubsection{Validation}
\label{sec:validation}

Validation tests ensure that new methods or algorithms proposed within \exama deliver meaningful improvements in real-world, production-level scenarios.

\paragraph{Process:}
\begin{itemize}
    \item Evaluate the proposed algorithm in a realistic or representative configuration of the target scientific problem.
    \item Integrate validation cases with well-defined criteria to measure the contribution of the new method, whether in terms of accuracy, performance, or scalability.
\end{itemize}

\subsection{Data Management and I/O Bottlenecks}
\label{sec:data-management}

As we advance towards exascale computing, efficiently managing massive datasets becomes a critical challenge. I/O bottlenecks are closely linked to several key issues, including \ac{B6}, \ac{B9}, \ac{B10}, and \ac{B11}. 
These bottlenecks arise from both offensive I/O (e.g., data analysis and compression) and defensive I/O (e.g., fault tolerance and resilience against failure).

To address these challenges, our methodology includes the following strategies:
\begin{itemize}
    \item Leveraging high-performance parallel file systems to enhance data throughput and reduce I/O latency across distributed architectures.
    \item Employing advanced data compression techniques to reduce the size of datasets without sacrificing accuracy, thus speeding up data movement and analysis.
    \item Implementing fault tolerance mechanisms to ensure data integrity and resilience in the face of hardware or software failures.
    \item Integrating efficient I/O scheduling and buffering techniques to avoid I/O becoming a performance bottleneck during large-scale simulations.
\end{itemize}

Our benchmarks will assess not only computational performance but also how efficiently data is read, written, and transferred across the system. This will help ensure that I/O does not become a limiting factor for scalability and performance. The analysis of I/O performance will be carried out under varying workloads to test both offensive and defensive I/O strategies, providing insights into the resilience and reproducibility of the system across different HPC environments.

\subsection{Fault Tolerance Strategy}
\label{sec:fault-tolerance-strategy}

As we transition towards exascale computing, ensuring fault tolerance is very important to maintain the resilience and reliability of both the system and the scientific applications that run on it. The increasing scale and complexity of exascale systems introduce challenges related to hardware failures, software errors, and data integrity issues that could disrupt large-scale computations. To address these challenges, \ac{exama} will develop a  strategy that incorporates fault tolerance at multiple levels, including application design, data management, and build upon solutions from system architecture.

The general approach will be to ensure that applications can continue to operate in the presence of failures by employing techniques such as \textbf{Checkpoint/Restart} for state preservation, failure detection, and system recovery. Beyond traditional fault tolerance methods, \ac{exama} aims to integrate advanced mechanisms that allow for more granular error handling, reducing the need for full system restarts and improving the overall efficiency and robustness of computations.

The strategy will also include leveraging fault-tolerant I/O frameworks and resilient data storage solutions to ensure that data integrity is maintained across all phases of computation, from input/output operations to long-term storage. Through these efforts, \ac{exama} seeks to provide a fault-tolerant infrastructure capable of sustaining exascale workloads under increasingly challenging conditions.


\subsection{Demonstrator Framework}
\label{sec:demonstrators}

Each demonstrator in the \exama project must be documented and subjected to the following criteria:

\subsubsection{Demonstrator Details}
\begin{itemize}
    \item \textbf{Name and Description:} Briefly describe the demonstrator, including its scientific significance and the computational barriers it seeks to overcome.
    \item \textbf{Software Components:} List the software tools involved and the challenges faced, including bottlenecks (memory, algorithms, data management).
    \item \textbf{Associated Work Packages:} Identify the work packages relevant to the demonstrator.
\end{itemize}

\subsubsection{Test Classification}
Each demonstrator must define a set of tests:
\begin{itemize}
    \item \textbf{Test Name:} Specify the name and type of the test (non-regression, verification, or validation).
    \item \textbf{Hardware Platform:} CPU, GPU, or hybrid system.
    \item \textbf{Outputs and Tolerances:} Define the expected outputs and the acceptable deviations from the reference solution.
\end{itemize}

\paragraph{Demonstrator Levels:}
\begin{itemize}
    \item \textbf{Level 1:} Involves one or two work packages (WP1, WP2, etc.).
    \item \textbf{Level 2:} Spans three or four work packages.
    \item \textbf{Level 3:} Covers the entire scope of \exama, potentially involving all work packages.
\end{itemize}

\subsection{Containerization and Software Packaging}
\label{sec:containerization}

To ensure reproducibility and simplify the deployment process, each demonstrator will be packaged using container technologies like Docker, Apptainer, Spack, or Guix-HPC. The process is as follows:

\paragraph{Container Image:}
Each demonstrator will have its own container image containing:
\begin{itemize}
    \item The operating system and necessary dependencies.
    \item Instructions for retrieving the demonstrator source code.
    \item Compilation and installation commands for the demonstrator.
    \item Execution of tests to verify correct behavior.
\end{itemize}

This approach ensures that software can be deployed in a variety of environments without worrying about system-specific dependencies, simplifying both testing and scaling efforts.

\subsection{Reproducibility and Sustainability}
\label{sec:reproducibility}

One of the core principles of the \exama project is ensuring long-term reproducibility and sustainability of the software solutions produced. This is achieved through the following processes:
\begin{itemize}
    \item CI/CD pipelines.
    \item Continuous benchmarking and regression testing on new hardware platforms.
    \item Ensuring that the new methods provide measurable improvements over time.
    \item Cross-verification of software across multiple \exama and \numpex work packages.
\end{itemize}

\subsection{Conclusion}
\label{sec:methodology-conclusion}

This chapter has presented a detailed methodology for benchmarking, testing, and validating software developed within \exama. It builds on core principles such as non-regression, verification, and validation, while incorporating modern CI/CD, packaging and containerization practices and continuous benchmarking. 
This is an initial version that will certainly evolve. 
\exama will implement it during the coming month and report its success in the next version of the deliverable.
Future versions of this methodology will evolve based on the experiences gained through ongoing development and real-world application of the \exama software.

\section{Concretization and Technologies}
\label{sec:benchmark:technologies}

We now turn to some concrete technologies that will help executing the Methodology.

\subsection{Types of Benchmarking}
\label{sec:methodology-types}

\paragraph{CPU or GPU}
Software that can run on either CPU or GPU but not both simultaneously (e.g., PyTorch, SciMba). In this case, benchmarks will be performed on both CPU and GPU, but no single run will execute on both architectures simultaneously. Some benchmarks may be CPU-only, others GPU-only, depending on the software's capabilities.

\paragraph{CPU and GPU}
Software that supports simultaneous execution on both CPU and GPU during a single simulation run (e.g., TRUST). Benchmarks in this category will involve executing components on both CPU and GPU, ensuring that at least one computational component runs on each architecture.

\paragraph{Explanation of Benchmarking Criteria}
\begin{description}
    \item[Pure Performance Benchmarks] Description of benchmarks based on computational performance, including metrics such as execution time and FLOPS.

    \item[Scalability Benchmarks] Discussion of scalability tests, including weak and strong scaling tests, and their relevance for exascale applications.
    \item[Energy Efficiency Benchmarks]Methodologies for measuring energy efficiency, energy profiling tools, and associated metrics (e.g., energy consumption per operation).
    \item[Hybrid Benchmarks] Strategies for benchmarking hybrid CPU-GPU codes, including tools for profiling and performance measurement.
    
\end{description}

\subsection{Profiling and Performance Measurement Tools}
\label{sec:methodology-tools}
This section presents the tools used to collect profiling data and analyze the performance of codes. 

A comparison of the advantages of each tool for different types of benchmarks is also provided.



\subsubsection{Extrae for CPU Architectures}
\label{sec:methodology-tools-extrae}

\textbf{Extrae} is a performance analysis tool developed by the Barcelona Supercomputing Center (BSC) designed to instrument and profile parallel applications. 
It is particularly useful for applications using MPI, OpenMP, CUDA, OpenCL, or hybrid programming models. 
Extrae collects detailed execution traces of an application, which can then be visualized and analyzed using tools like Paraver or Vampir~\ref{sec:methodology-tools-vampir}.
 This helps developers identify performance bottlenecks, communication inefficiencies, and parallelization issues.

The key features of Extrae include:

\begin{itemize}
  \item \textbf{Support for Multiple Programming Models:} Extrae can instrument applications using MPI, OpenMP, CUDA, OpenCL, pthreads, and combinations thereof.
  \item \textbf{Detailed Tracing:} Captures function calls, communication events, synchronization points, and hardware counters.
  \item \textbf{Customization:} Users can specify what events to trace and which metrics to collect through configuration files.
  \item \textbf{Integration with Visualization Tools:} Generates trace files compatible with Paraver and Vampir for in-depth analysis.
\end{itemize}


\subsubsection{Score-P}
\label{sec:methodology-tools-scorep}

\begin{itemize}
    \item CPU: Score-P is extensively used for CPU performance profiling, particularly in distributed memory systems using MPI, OpenMP, or hybrid programming models.
    \item GPU: Score-P also supports GPU profiling, including CUDA and OpenCL applications. It can capture the performance of both the host (CPU) and the device (GPU), including offloading, kernel execution times, and memory transfers.
\end{itemize}


\subsubsection{TAU}
\label{sec:methodology-tools-tau}

\begin{itemize}
    \item CPU: TAU is widely used for profiling and tracing CPU-bound applications, with support for both shared and distributed memory parallelism (e.g., OpenMP, MPI).
    \item GPU: TAU supports GPU profiling, particularly for CUDA-based applications. It collects performance metrics from both the host (CPU) and the device (GPU), allowing for an analysis of hybrid codes. TAU supports CUDA, OpenCL, and OpenACC, making it suitable for GPU-accelerated applications.
\end{itemize}


\subsubsection{Vampir}
\label{sec:methodology-tools-vampir}

\begin{itemize}
    \item CPU: Vampir is a popular tool for visualizing performance data collected from CPU-based applications, particularly those running in parallel using MPI and OpenMP.
    \item GPU: Vampir can also visualize GPU-related performance data when combined with Score-P, as Score-P collects traces from both the CPU and the GPU. It can visualize events such as kernel execution, memory transfers, and CUDA API calls.
\end{itemize}


\subsubsection{Nsight for GPU Architectures}
\label{sec:methodology-tools-nsight}

Nsight\footnote{\url{https://developer.nvidia.com/nsight-systems}} is a set of tools developed by NVIDIA for profiling and debugging on GPU architectures. 
It allows for a detailed performance analysis of CUDA-based codes, including metrics such as occupancy, execution time, and memory throughput.
Nsight also provides visualizations that help pinpoint bottlenecks in GPU applications.

\subsubsection{Arm Map for CPU Architectures}
\label{sec:methodology-tools-armmap}

Arm Map\footnote{\url{https://developer.arm.com/documentation/102732/latest/}} is a lightweight and highly scalable profiler designed specifically for CPU architectures. 
It provides insights into time spent in computation, communication, and memory access. 
Arm Map can visualize CPU-bound performance bottlenecks and assist in optimizing codes for multi-core CPU systems.

\subsubsection{PETSc \texttt{-log\_view} for PETSc-based Codes}
\label{sec:methodology-tools-petsc}

PETSc~\cite{balay_petsc_2024} provides built-in profiling options via the \texttt{-log\_view} flag~\cite{balay_petsctao_2024}. 
This option enables users to gather detailed performance metrics such as function timings, memory usage, and communication patterns for codes based on the PETSc library. 

For CPU-based codes, \texttt{-log\_view} captures the performance data related to CPU function calls and overall computation, including the performance of solvers, preconditioners, and communication overhead in parallel systems.

For GPU-based codes, the \texttt{-log\_view\_gpu\_time} option is used to gather profiling information specifically for GPU activities. 
This flag tracks kernel execution times, memory transfers between host and device, and other GPU-related performance metrics, allowing for in-depth analysis of GPU-accelerated PETSc applications.




\subsection{Regression Testing and Verification}
\label{sec:methodology-regression}

\subsubsection{ReFrame}
\label{sec:methodology-regression-reframe}


ReFrame~\cite{karakasis_reframe-hpcreframe_2024} is a regression testing and benchmarking framework specifically designed for high-performance computing (HPC) systems. 
Developed and maintained by the Swiss National Supercomputing Centre (CSCS), it is optimized for managing system tests and performance benchmarks. 
ReFrame provides an abstraction layer that decouples the test logic from the specifics of the system environment, allowing seamless execution of benchmarks across multiple HPC platforms.

\subsubsection{Key Features and Design}

ReFrame's core functionality is centered around its modular, Python-based architecture, which provides extensive flexibility and control over test configurations and execution pipelines. The main features of ReFrame include:

\begin{itemize}
    \item \textbf{System Abstraction:} ReFrame abstracts system-specific details such as compilers, libraries, and modules, enabling users to define system-independent test logic. This abstraction simplifies portability across various HPC environments.
    \item \textbf{Python-Based Test Definitions:} All test logic in ReFrame is defined in Python, allowing for high customizability and the use of standard Python libraries. Tests are implemented as Python functions, which manage the execution of benchmarks, result validation, and post-processing.
    \item \textbf{Configuration and Execution Separation:} The framework separates the system configuration from the test logic. The configuration file defines system properties, including partitions, compilers, and runtime environments. The test file, on the other hand, defines the logic of the tests, including the execution stages and result handling.
    \item \textbf{Pipeline Flexibility:} ReFrame supports the creation of complex test pipelines through its modular structure. It allows defining pre- and post-processing stages, intra-test dependencies, and validation steps, making it suitable for a wide range of benchmarking scenarios.
\end{itemize}


The proposal is to use ReFrame  to benchmark the project's software and libraries on a variety of HPC systems. 
The framework's ability to provide system-independent test logic is crucial in this respect, as the project aims to deploy and benchmark  on multiple heterogeneous HPC systems. 
This ensures consistent benchmarking across platforms with varying architectures, compilers, and parallel frameworks.

The test structure in ReFrame consists of the following key components:

\begin{itemize}
    \item \textbf{Configuration File:} Defines the HPC systems and their environments, including compilers, libraries, and runtime modules. A new configuration file or an extension to an existing one is required when deploying tests on a different system.
    \item \textbf{Test File:} Contains the core test logic, including benchmark execution and validation stages. The test file remains system-independent, allowing the same test to be deployed on multiple platforms.
    \item \textbf{Post-Processing:} ReFrame provides built-in support for result collection and validation. Benchmark results are parsed and stored in a standardized format, facilitating consistent performance reporting across different HPC environments.
\end{itemize}

ReFrame ensures reproducibility, scalability, and portability in Exa-MA benchmarking efforts, making it thus a central tool in managing the project's performance evaluation across various supercomputing resources.

\subsection{Fault Tolerance Mechanisms}
\label{sec:fault-tolerance}

As we move towards exascale computing, ensuring fault tolerance becomes critical to maintaining system resilience and data integrity in the face of hardware or software failures. However, currently, only a limited number of software tools within \ac{exama} support traditional \textbf{Checkpoint/Restart} techniques, which periodically save the application state to enable recovery from failures. This limitation presents a bottleneck in achieving full fault tolerance across all systems. 

HDF5 has been adopted by various software within the \exama project as a standard data format for managing large-scale datasets, see pie chart~\ref{fig:data}. Its robust parallel I/O capabilities, combined with fault-tolerant extensions, make it a key framework for ensuring data integrity and resilience at exascale. We should leverage the fault-tolerant features of \textbf{HDF5}, such as its fault-tolerant parallel I/O extensions~\cite{koziol_extreme_2012}, to address potential failures during data read and write operations in our benchmarks and applications.

Looking forward, \ac{exama} aims to integrate more advanced fault tolerance mechanisms over the course of the project. These will include cutting-edge technologies such as \textbf{OpenMPI-ULFM} (User-Level Failure Mitigation)~\cite{open_mpi_documentation_team_user_2024,fault_tolerance_working_group_mpi_forum_user_2024} and the \textbf{FTI} (Fault Tolerance Interface)~\cite{bautista-gomez_fti_2011,fti_documentation_team_fti_2024} which provide more sophisticated failure detection and recovery capabilities in MPI applications. These technologies will allow for fine-grained error handling and recovery without requiring a complete restart of the system.


In addition, future developments will explore scalable checkpointing solutions like \textbf{SCR} (Scalable Checkpoint/Restart)~\cite{laboratory_llnl_scalable_2024}, which optimize checkpointing by utilizing local storage and reduce I/O overhead, thus improving recovery times. Depending on the availability, we might explore supporting resilient data management systems such as \textbf{ZFS} (Zettabyte File System)~\cite{project_zfs_2024} or \textbf{DAOS} (Distributed Asynchronous Object Storage)~\cite{liang_daos_2020}.



\subsection{Packaging and Containerization}
\label{sec:methodology-packaging}

\subsubsection{Spack}
\label{sec:methodology-packaging-spack}

Spack~\cite{gamblin_spack_2015} is a flexible package management tool used to simplify the installation of complex HPC software environments. It allows users to create and manage multiple versions of software libraries and applications, ensuring compatibility across different systems.

\subsubsection{Guix-HPC}
\label{sec:methodology-packaging-guix-hpc}

Guix-HPC~\cite{vallet_toward_2022}, an advanced package management system tailored for HPC environments. Guix-HPC ensures reproducibility and scalability by providing a declarative environment specification. By using Guix, we can control software dependencies and build environments across different HPC systems, ensuring that benchmarks and experiments can be replicated with exact configurations.


\subsubsection{Containers}
\label{sec:methodology-packaging-container}

Methods of containerization to ensure the portability and reproducibility of benchmarking environments.
The use of Docker and Singularity to encapsulate dependencies and simplify deployment on different infrastructures is discussed.

\subsection{Presentation of Results}
\label{sec:methodology-presentation}

The presentation of benchmarking results is important for clearly communicating performance metrics and insights derived from their analysis. Standard formats for presenting these results include the following:

\paragraph{Performance Graphs:} Performance graphs such as line plots, bar charts, and scatter plots are used to visualize key metrics like execution time, floating-point operations per second (FLOPS), memory usage, and scalability (e.g., strong and weak scaling). These graphs provide a visual comparison of the software's performance across different architectures (CPU, GPU, or hybrid setups), problem sizes, and parallel configurations. Graphs may also be used to display energy efficiency metrics, such as power consumption over time or energy-to-solution.

\paragraph{Comparative Tables:} Comparative tables are employed to present side-by-side comparisons of different software versions or configurations. These tables can include metrics such as execution time, memory usage, cache utilization, FLOPS, and energy consumption. Comparative tables offer a quick overview of how different systems, algorithms, or configurations perform under the same conditions.

\paragraph{Summary Reports:} Summary reports encapsulate the key findings of the benchmarking activities. These reports include both qualitative and quantitative evaluations of performance, scalability, and energy efficiency. A summary report typically includes a discussion of bottlenecks, identified challenges, and opportunities for optimization. Additionally, tables and graphs from the benchmarking process are integrated into the report for the analysis of the results.


\subsection{Scalability and Hardware Environments}
\label{sec:methodology-environments}

This section describes the hardware architectures used in the benchmarks:
\begin{itemize}
    \item \textbf{CPU Architectures:} Multi-core CPU systems (Intel, AMD, ARM) with associated performance metrics such as scalability, memory bandwidth, and computational throughput.
    \item \textbf{GPU Architectures:} NVIDIA and AMD GPUs, focusing on metrics such as memory latency, occupancy, and computational throughput using CUDA or HIP.
    \item \textbf{Hybrid Systems:} Systems combining both CPU and GPU architectures, analyzing the balance of workloads between the two and exploring tools that optimize for such hybrid environments.
\end{itemize}
Tools for resource management, such as job scheduling systems (e.g., SLURM), are also discussed.


\section{Conclusion}
\label{sec:Technology-conclusion}

This chapter outlines a methodology for benchmarking, testing, and validating the software developed within the \exama project. By addressing key bottlenecks such as \ac{B2} (interconnect technology), \ac{B3} (memory technology), \ac{B6} (data management), \ac{B7} (exascale algorithms), and \ac{B11} (reproducibility), the methodology focuses on scalability, performance, reproducibility, and fault tolerance, which are critical for exascale readiness.




We presented a phased benchmarking strategy that integrates testing, validation, profiling, and continuous integration to ensure that the software is ready for exascale environments. The testing processes, including non-regression, verification, and validation, emphasize maintaining performance integrity across updates and ensuring that new algorithmic improvements lead to meaningful progress.

The benchmarking strategy involves comprehensive testing across \textbf{CPU}, \textbf{GPU}, and \textbf{hybrid} CPU-GPU architectures to ensure that software can scale efficiently. This includes separate tests for CPU and GPU performance, as well as hybrid configurations where workloads are distributed across both architectures. 

Data management and I/O strategies (\ac{B6}, \ac{B9}, \ac{B10}, \ac{B11}) were also highlighted to address I/O bottlenecks and ensure efficient handling of large datasets, leveraging fault-tolerant mechanisms for data integrity. As the project progresses, more advanced fault tolerance strategies may be integrated to ensure resilience against hardware or software failures.

In the Concretization and Technologies section, key tools for benchmarking, such as Extrae, Score-P, and Vampir, were introduced along with packaging and containerization technologies like Spack and Docker, which ensure portability and reproducibility across diverse HPC environments. 

Finally, this methodology aligns with the \ac{FAIR} data principles, ensuring that benchmark results and datasets are \textbf{Findable}, \textbf{Accessible}, and \textbf{Reusable}. However, full \textbf{Interoperability} is certainly not yet ready. Additional efforts will focus on supporting standard data formats associated with rich metadata. By implementing these extra measures, the \exama project can then enable benchmarks and  their data to be shared between different software, facilitating collaborative work and, hence, extending the impact of the benchmarking activities.

Future versions of this methodology will evolve based on ongoing testing and real-world applications of the \ac{exama} software.
